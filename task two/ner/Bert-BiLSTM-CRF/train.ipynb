{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf \n",
    "from bert4keras.backend import K,keras,search_layer\n",
    "from bert4keras.snippets import ViterbiDecoder,to_array\n",
    "\n",
    "from data_load import *\n",
    "from build_model import bert_bilstm_crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# 固定随机种子\n",
    "seed = 233\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHSHSEED'] = str(seed)\n",
    "\n",
    "# 权重参数\n",
    "epochs = 4\n",
    "batch_size = 16\n",
    "lstm_units = 128\n",
    "drop_rate = 0.1 #有改动0.1-》0.01\n",
    "learning_rate = 5e-5\n",
    "max_len =168\n",
    "\n",
    "#精细训练\n",
    "fine_train_list = [0 for i in range(8275)]\n",
    "train_predict_list = []\n",
    "\n",
    "# 权重路径\n",
    "config_path = './bert_weight_file/uncased_L-4_H-768_A-12/bert_config.json'\n",
    "checkpoint_path = './bert_weight_file/uncased_L-4_H-768_A-12/bert_model.ckpt'\n",
    "\n",
    "# 模型保存路径\n",
    "model_save_path = './save_model/bert_bilstm_crf.weight'\n",
    "CRF_save_path = './save_model/CRF.npy'\n",
    "\n",
    "class NamedEntityRecognizer(ViterbiDecoder):\n",
    "    \"\"\"命名实体识别器\n",
    "    \"\"\"\n",
    "    def recognize(self, text):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        while len(tokens) > max_len:\n",
    "            tokens.pop(-2)\n",
    "        mapping = tokenizer.rematch(text, tokens)\n",
    "        token_ids = tokenizer.tokens_to_ids(tokens)\n",
    "        segment_ids = [0] * len(token_ids)\n",
    "        token_ids, segment_ids = to_array([token_ids], [segment_ids]) # ndarray\n",
    "        nodes = model.predict([token_ids, segment_ids])[0] # [sqe_len,23]\n",
    "        labels = self.decode(nodes) # id [sqe_len,], [0 0 0 0 0 7 8 8 0 0 0 0 0 0 0]\n",
    "        entities, starting = [], False\n",
    "        for i, label in enumerate(labels):\n",
    "            if label > 0:\n",
    "                if label % 2 == 1:\n",
    "                    starting = True\n",
    "                    entities.append([[i], id2label[(label - 1) // 2]])\n",
    "                elif starting:\n",
    "                    entities[-1][0].append(i)\n",
    "                else:\n",
    "                    starting = False\n",
    "            else:\n",
    "                starting = False\n",
    "        return [(text[mapping[w[0]][0]:mapping[w[-1]][-1] + 1], l) for w, l in entities]\n",
    "    \n",
    "#相等应加set（）中源文本的数量    \n",
    "def ner_metrics(data,fine_train_list):\n",
    "    X,Y,Z = 1e-6,1e-6,1e-6\n",
    "    count = 0\n",
    "    for d in tqdm(data):\n",
    "        text = ''.join([i[0] for i in d])\n",
    "        pred= NER.recognize(text)\n",
    "        R = set(pred)\n",
    "        T = set([tuple(i) for i in d if i[1] != 'O'])\n",
    "        \n",
    "        # 便于T和R做交集\n",
    "        m = []\n",
    "        for i in T:\n",
    "            for j in i[0]:\n",
    "                m.append((j,i[1]))\n",
    "        T = set(m)\n",
    "        \n",
    "        # 填充train_predict_list,更新fine_train_list\n",
    "        if len(T) > 0 :  \n",
    "            if len(train_predict_list) < 8275:\n",
    "                train_predict_list.append(R&T)\n",
    "            else:\n",
    "                if len(R&T) > fine_train_list[count]:\n",
    "#                     print('text: ',text)\n",
    "#                     print('T: ',T)\n",
    "#                     print('R&T: ',R&T)\n",
    "                    train_predict_list[count] = R&T\n",
    "            if len(R&T) > fine_train_list[count]:\n",
    "                fine_train_list[count] = len(R&T)\n",
    "            \n",
    "            \n",
    "#         if len(T) < fine_train_list[count]:\n",
    "#             print(False)\n",
    "#             print('text: ',text)\n",
    "#             print('T: ',T)\n",
    "#             print('R&T: ',R&T)\n",
    "#             print('fine_train_list[count]: ',fine_train_list[count])\n",
    "#             print()\n",
    "\n",
    "        X += fine_train_list[count]\n",
    "        if len(R) < fine_train_list[count]:\n",
    "            Y += fine_train_list[count]\n",
    "        else:\n",
    "            Y += len(R)\n",
    "            \n",
    "        Z += len(T)\n",
    "        count += 1\n",
    "\n",
    "    f1,precision,recall = 2 * X / (Y + Z),X / Y,X / Z\n",
    "    return f1,precision,recall\n",
    "\n",
    "class Evaluator(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(Evaluator, self).__init__()\n",
    "        self.best_val_f1 = 0\n",
    "    def on_epoch_end(self, epoch,logs=None):\n",
    "        NER.trans = K.eval(CRF.trans) # 可能有错\n",
    "        f1, precision, recall = ner_metrics(valid_data,fine_train_list)\n",
    "        if f1 > self.best_val_f1:\n",
    "            model.save_weights(model_save_path)\n",
    "            self.best_val_f1 = f1\n",
    "            print('save model to {}'.format(checkpoint_path))\n",
    "        else:\n",
    "            global learning_rate\n",
    "            learning_rate = learning_rate / 5\n",
    "        print(\n",
    "              'valid: f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f\\n' %\n",
    "              (f1,precision,recall,self.best_val_f1)\n",
    "        )\n",
    "        \n",
    "# def adversarial_training(model, embedding_name, epsilon=1):\n",
    "#     \"\"\"\n",
    "#     给模型添加对抗训练\n",
    "#     其中model是需要添加对抗训练的keras模型\n",
    "#     \"\"\"\n",
    "#     if model.train_function is None:  # 如果还没有训练函数\n",
    "#         model._make_train_function()  # 手动make\n",
    "#     old_train_function = model.train_function  # 备份旧的训练函数\n",
    "\n",
    "#     # 查找Embedding层\n",
    "#     for output in model.outputs:\n",
    "#         embedding_layer = search_layer(output, embedding_name)\n",
    "#         if embedding_layer is not None:\n",
    "#             break\n",
    "#     if embedding_layer is None:\n",
    "#         raise Exception('Embedding layer not found')\n",
    "\n",
    "#     # 求Embedding梯度\n",
    "#     embeddings = embedding_layer.embeddings  # Embedding矩阵\n",
    "#     gradients = K.gradients(model.total_loss, [embeddings])  # Embedding梯度\n",
    "#     gradients = K.zeros_like(embeddings) + gradients[0]  # 转为dense tensor\n",
    "\n",
    "#     # 封装为函数\n",
    "#     inputs = (\n",
    "#         model._feed_inputs + model._feed_targets + model._feed_sample_weights\n",
    "#     )  # 所有输入层\n",
    "#     embedding_gradients = K.function(\n",
    "#         inputs=inputs,\n",
    "#         outputs=[gradients],\n",
    "#         name='embedding_gradients',\n",
    "#     )  # 封装为函数\n",
    "\n",
    "#     def train_function(inputs):\n",
    "#         # 重新定义训练函数\n",
    "#         grads = embedding_gradients(inputs)[0]  # Embedding梯度\n",
    "#         delta = epsilon * grads / (np.sqrt((grads**2).sum()) + 1e-8)  # 计算扰动\n",
    "#         K.set_value(embeddings, K.eval(embeddings) + delta)  # 注入扰动\n",
    "#         outputs = old_train_function(inputs)  # 梯度下降\n",
    "#         K.set_value(embeddings, K.eval(embeddings) - delta)  # 删除扰动\n",
    "#         return outputs\n",
    "#     model.train_function = train_function  # 覆盖原训练函数        \n",
    "\n",
    "\n",
    "\n",
    "model,CRF = bert_bilstm_crf(config_path,checkpoint_path,num_labels,lstm_units,drop_rate,learning_rate)\n",
    "# adversarial_training(model,'Embedding-Token',0.5)\n",
    "NER = NamedEntityRecognizer(trans=K.eval(CRF.trans), starts=[0], ends=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/4\n",
      "3718/3718 [==============================] - 8359s 2s/step - loss: 3.5378 - sparse_accuracy: 0.9333 - val_loss: 2.4746 - val_sparse_accuracy: 0.9644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [03:30<00:00, 39.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model to ./bert_weight_file/uncased_L-4_H-768_A-12/bert_model.ckpt\n",
      "valid: f1: 0.53504, precision: 0.56155, recall: 0.51091, best f1: 0.53504\n",
      "\n",
      "Epoch 2/4\n",
      "3718/3718 [==============================] - 8403s 2s/step - loss: 2.3291 - sparse_accuracy: 0.9467 - val_loss: 0.8363 - val_sparse_accuracy: 0.9634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [03:15<00:00, 42.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: f1: 0.53315, precision: 0.51467, recall: 0.55300, best f1: 0.53504\n",
      "\n",
      "Epoch 3/4\n",
      "3718/3718 [==============================] - 8421s 2s/step - loss: 1.8528 - sparse_accuracy: 0.9514 - val_loss: 1.6299 - val_sparse_accuracy: 0.9653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [03:18<00:00, 41.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model to ./bert_weight_file/uncased_L-4_H-768_A-12/bert_model.ckpt\n",
      "valid: f1: 0.55507, precision: 0.55009, recall: 0.56014, best f1: 0.55507\n",
      "\n",
      "Epoch 4/4\n",
      "3718/3718 [==============================] - 8365s 2s/step - loss: 1.7498 - sparse_accuracy: 0.9525 - val_loss: 1.2923 - val_sparse_accuracy: 0.9659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [03:19<00:00, 41.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model to ./bert_weight_file/uncased_L-4_H-768_A-12/bert_model.ckpt\n",
      "valid: f1: 0.55734, precision: 0.55372, recall: 0.56099, best f1: 0.55734\n",
      "\n",
      "[[ 0.4287106  -0.86728936  0.1629885   0.22899823]\n",
      " [-0.33825907 -0.543957   -0.55285895 -0.7845749 ]\n",
      " [ 0.17082854 -0.9606606  -0.27878678 -0.5104652 ]\n",
      " [-0.8264545  -0.555815    0.37197393  0.81980103]]\n",
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_data,_ = load_data('./data/train/train.txt',128)\n",
    "    valid_data,_ = load_data('./data/test/test.txt',128)\n",
    "    \n",
    "    flag = False\n",
    "    count = 0\n",
    "    i = 0\n",
    "    while(i<len(train_data)):\n",
    "        if flag==True:\n",
    "            i = i-1\n",
    "        if train_data[i][0][1] == 'O'and len(train_data[i])==1:\n",
    "            del train_data[i]\n",
    "            flag = True\n",
    "            count+=1\n",
    "        else:\n",
    "            for j in range(count):\n",
    "                train_data.append(train_data[i])\n",
    "            flag = False\n",
    "            count = 0\n",
    "        i += 1\n",
    "    \n",
    "    train_generator = data_generator(train_data, batch_size)\n",
    "    valid_generator = data_generator(valid_data, batch_size*5)\n",
    "    \n",
    "    evaluator = Evaluator()\n",
    "    \n",
    "    def scheduler(epoch):\n",
    "        return learning_rate/(max(2*(epoch-1),1))\n",
    "\n",
    "    lr_scheduler = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    \n",
    "    model.fit(\n",
    "        train_generator.forfit(),\n",
    "        steps_per_epoch = len(train_generator),\n",
    "        validation_data = valid_generator.forfit(),\n",
    "        validation_steps = len(valid_generator),\n",
    "        epochs = epochs,\n",
    "        callbacks = [evaluator,lr_scheduler]\n",
    "    )\n",
    "    \n",
    "    print(K.eval(CRF.trans))\n",
    "    print(K.eval(CRF.trans).shape)\n",
    "    model.save_weights(model_save_path)\n",
    "    np.save(CRF_save_path, K.eval(CRF.trans))\n",
    "\n",
    "    # torch.save(model, model_save_path)\n",
    "    # pickle.dump(K.eval(CRF.trans),open('./save_model/crf_trans.pkl','rb'))\n",
    "    \n",
    "else:\n",
    "    # model = torch.load(model_save_path)\n",
    "    model.load_weights(model_save_path)\n",
    "    # NER.trans = pickle.load(open('./save_model/crf_trans.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_data,_ = load_data('./data/train/train.txt',128)\n",
    "    valid_data,_ = load_data('./data/test/test.txt',128)\n",
    "    \n",
    "\n",
    "    \n",
    "    train_generator = data_generator(train_data, batch_size)\n",
    "    valid_generator = data_generator(valid_data, batch_size*5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['医生：你好我是您的接诊医生', 'O']], [['医生：宝贝最近吃奶量可以吗？下降了吗', 'O']], [['患者：没有，也没怎么', 'O'], ['哭闹', 'Symptom-0']], [['医生：宝妈有没有吃生冷辛辣刺激食物油腻食物来吗？', 'O']], [['医生：宝贝奶粉的话最近换过牌子吗？', 'O']], [['医生：宝贝肚子着凉来吗？', 'O']], [['患者：喝茶油腻也少，菜吃很多', 'O']], [['医生：嗯嗯，宝妈饮食一定注意，生冷辛辣刺激食物不能吃油腻食物不能吃，特别油腻食物的奥，清淡饮食为主，这个时候宝贝胃肠功能可能会有影响，能吃多少吃多少别强喂的奥！', 'O']], [['医生：宝贝最近有没有', 'O'], ['呕吐', 'Symptom-1'], ['症状呢？', 'O']], [['患者：', 'O'], ['呕吐', 'Symptom-1'], ['，有时会', 'O'], ['吐', 'Symptom-1'], ['，不多', 'O']]]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0:10])\n",
    "# print(train_predict_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 保存矩阵\n",
    "fine=np.array(fine_train_list)\n",
    "tpl = np.array(train_predict_list)\n",
    "np.save('./fine_train_list.npy',fine)\n",
    "np.save('./train_predict_list.npy',tpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载矩阵\n",
    "fine = np.load('./fine_train_list.npy')\n",
    "fine_train_list = fine.tolist()\n",
    "tpl = np.load('./train_predict_list.npy',allow_pickle=True)\n",
    "train_predict_list = tpl.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3718/3718 [==============================] - 7081s 2s/step - loss: 0.7419 - sparse_accuracy: 0.9763 - val_loss: 0.7400 - val_sparse_accuracy: 0.9701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [03:33<00:00, 38.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model to ./bert_weight_file/uncased_L-4_H-768_A-12/bert_model.ckpt\n",
      "valid: f1: 0.58730, precision: 0.60555, recall: 0.57012, best f1: 0.58730\n",
      "\n",
      "Epoch 2/3\n",
      "3718/3718 [==============================] - 7173s 2s/step - loss: 0.6346 - sparse_accuracy: 0.9774 - val_loss: 0.7312 - val_sparse_accuracy: 0.9714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [03:37<00:00, 38.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model to ./bert_weight_file/uncased_L-4_H-768_A-12/bert_model.ckpt\n",
      "valid: f1: 0.59419, precision: 0.61280, recall: 0.57669, best f1: 0.59419\n",
      "\n",
      "Epoch 3/3\n",
      "3718/3718 [==============================] - 7183s 2s/step - loss: 0.5544 - sparse_accuracy: 0.9783 - val_loss: 1.0594 - val_sparse_accuracy: 0.9726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [03:35<00:00, 38.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model to ./bert_weight_file/uncased_L-4_H-768_A-12/bert_model.ckpt\n",
      "valid: f1: 0.59558, precision: 0.60751, recall: 0.58411, best f1: 0.59558\n",
      "\n",
      "[[ 0.20985383 -1.045513   -0.256569   -0.10786816]\n",
      " [-0.5771153  -0.427747   -0.9554797  -1.1635962 ]\n",
      " [-0.2451467  -1.3626031  -0.6572353  -0.9109553 ]\n",
      " [-1.064843   -0.96952844 -0.03134646  0.8583275 ]]\n",
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epochs = 3\n",
    "    \n",
    "    train_data,_ = load_data('./data/train/train.txt',128)\n",
    "    valid_data,_ = load_data('./data/test/test.txt',128)\n",
    "    \n",
    "\n",
    "    train_generator = data_generator(train_data, batch_size)\n",
    "    valid_generator = data_generator(valid_data, batch_size*5)\n",
    "    \n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        model_save_path,\n",
    "        monitor = 'val_sparse_accuracy',\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        mode = 'max'\n",
    "    )\n",
    "    evaluator = Evaluator()\n",
    "    \n",
    "#     def scheduler(epoch):\n",
    "#         return learning_rate/(max(2*(epoch-1),1))\n",
    "\n",
    "#     lr_scheduler = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    \n",
    "    model.fit(\n",
    "        train_generator.forfit(),\n",
    "        steps_per_epoch = len(train_generator),\n",
    "        validation_data = valid_generator.forfit(),\n",
    "        validation_steps = len(valid_generator),\n",
    "        epochs = epochs,\n",
    "        callbacks = [evaluator]\n",
    "    )\n",
    "    \n",
    "    print(K.eval(CRF.trans))\n",
    "    print(K.eval(CRF.trans).shape)\n",
    "    model.save_weights(model_save_path)\n",
    "    np.save(CRF_save_path, K.eval(CRF.trans))\n",
    "\n",
    "    # torch.save(model, model_save_path)\n",
    "    # pickle.dump(K.eval(CRF.trans),open('./save_model/crf_trans.pkl','rb'))\n",
    "    \n",
    "else:\n",
    "    # model = torch.load(model_save_path)\n",
    "    model.load_weights(model_save_path)\n",
    "    # NER.trans = pickle.load(open('./save_model/crf_trans.pkl','rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
