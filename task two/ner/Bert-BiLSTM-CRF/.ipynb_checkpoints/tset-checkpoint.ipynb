{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一定要避免生冷食物也就是饭菜一定要热一些。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "file_path = \"./example.txt\"\n",
    "# Open the JSON file\n",
    "with open('D:\\dataset/train.json', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "data_index = list(data)\n",
    "# Extract the 'sentence' value from the JSON file\n",
    "text = ''\n",
    "for i in range(1630):\n",
    "    for j in range(len(list(data[data_index[i]]['dialogue']))):\n",
    "        sentence = data[data_index[i]]['dialogue'][j]['sentence']\n",
    "        text += ''.join(sentence)\n",
    "        text += ''.join('\\n')\n",
    "    text += ''.join('\\n')\n",
    "\n",
    "        \n",
    "with open(file_path, \"w\") as file:\n",
    "# Write the text to the file\n",
    "    file.write(text)\n",
    "    \n",
    "# Print the extracted sentence\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "f=open('./text.txt', encoding='utf-8')\n",
    "line = f.readline().strip() #读取第一行\n",
    "sentence1 =[]\n",
    "sentence1.append(line)\n",
    "while line:  # 直到读取完文件\n",
    "    line = f.readline().strip()  # 读取一行文件，包括换行符\n",
    "    sentence1.append(line)\n",
    "f.close()  # 关闭文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "f=open('./near_data/dev/input.txt', encoding='utf-8')\n",
    "line = f.readline().strip() #读取第一行\n",
    "sentence1 =[]\n",
    "sentence1.append(line)\n",
    "while line:  # 直到读取完文件\n",
    "    line = f.readline().strip()  # 读取一行文件，包括换行符\n",
    "    sentence1.append(line)\n",
    "f.close()  # 关闭文件\n",
    "\n",
    "f=open('./near_data/dev/output.txt', encoding='utf-8')\n",
    "line = f.readline().strip() #读取第一行\n",
    "labels1 =[]\n",
    "labels1.append(line)\n",
    "while line:  # 直到读取完文件\n",
    "    line = f.readline().strip()  # 读取一行文件，包括换行符\n",
    "    labels1.append(line)\n",
    "f.close()  # 关闭文件\n",
    "\n",
    "f=open('./near_data/train/input.txt', encoding='utf-8')\n",
    "line = f.readline().strip() #读取第一行\n",
    "sentence2 =[]\n",
    "sentence2.append(line)\n",
    "while line:  # 直到读取完文件\n",
    "    line = f.readline().strip()  # 读取一行文件，包括换行符\n",
    "    sentence2.append(line)\n",
    "f.close()  # 关闭文件\n",
    "\n",
    "f=open('./near_data/train/output.txt', encoding='utf-8')\n",
    "line = f.readline().strip() #读取第一行\n",
    "labels2 =[]\n",
    "labels2.append(line)\n",
    "while line:  # 直到读取完文件\n",
    "    line = f.readline().strip()  # 读取一行文件，包括换行符\n",
    "    labels2.append(line)\n",
    "f.close()  # 关闭文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path,max_len):\n",
    "    \n",
    "    sentence = []\n",
    "    labels = []\n",
    "    X = []\n",
    "    y = []\n",
    "    datasets = []\n",
    "    samples_len = []\n",
    "    split_pattern = re.compile(r'[;；。，、？\\.\\?!]')\n",
    "    with open(data_path,'r',encoding= 'utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip().split()\n",
    "            if(not line or len(line) < 2):\n",
    "                X.append(sentence.copy())\n",
    "                y.append(labels.copy())\n",
    "                sentence.clear()\n",
    "                labels.clear()\n",
    "                continue\n",
    "            word, tag = line[0], line[1]\n",
    "            if split_pattern.match(word) and len(sentence) >= max_len:\n",
    "                sentence.append(word)\n",
    "                labels.append(tag)\n",
    "                sentence.clear()\n",
    "                labels.clear()\n",
    "            else:\n",
    "                sentence.append(word)\n",
    "                labels.append(tag)\n",
    "    if len(sentence):\n",
    "        X.append(sentence.copy())\n",
    "        sentence.clear()\n",
    "        y.append(labels.copy())\n",
    "        labels.clear()\n",
    "\n",
    "    for token_seq,label_seq in zip(X,y):\n",
    "        if len(token_seq) < 2:\n",
    "            continue\n",
    "        sample_seq, last_flag = [], ''\n",
    "        for token, this_flag in zip(token_seq,label_seq):\n",
    "            \n",
    "            if this_flag == 'O' and last_flag == 'O':\n",
    "                sample_seq[-1][0] += token\n",
    "            elif this_flag == 'O' and last_flag != 'O':\n",
    "                sample_seq.append([token, 'O'])\n",
    "            elif this_flag[:1] == 'B':\n",
    "                sample_seq.append([token, this_flag[2:]]) \n",
    "                save = token\n",
    "            elif this_flag[:1] == 'I' and last_flag[:1] == 'B':\n",
    "                del sample_seq[-1][-1] \n",
    "                del sample_seq[-1][-1] \n",
    "                sample_seq.append([save+token, this_flag[2:]])\n",
    "                save = save+token\n",
    "            elif this_flag[:1] == 'I' and last_flag[:1] == 'I':\n",
    "                del sample_seq[-1][-1] \n",
    "                del sample_seq[-1][-1]\n",
    "                sample_seq.append([save+token, this_flag[2:]])\n",
    "                save = save+token\n",
    "            last_flag = this_flag\n",
    "        datasets.append([x for x in sample_seq if x != []])       \n",
    "        samples_len.append(len(token_seq))\n",
    "    \n",
    "    df = pd.DataFrame(samples_len)\n",
    "    print(data_path,'\\n',df.describe())\n",
    "    return datasets,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-731e865a9956>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']),\n\u001b[0;32m     31\u001b[0m                                                \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_encodings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'attention_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                                                torch.tensor(train_labels))\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m val_dataset = torch.utils.data.TensorDataset(torch.tensor(val_encodings['input_ids']),\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pandas as pd\n",
    "from bert4keras.snippets import sequence_padding,DataGenerator\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "\n",
    "# 加载训练数据\n",
    "\n",
    "train_texts = sentence1\n",
    "train_labels = labels1\n",
    "val_texts = sentence2\n",
    "val_texts = labels2 \n",
    "# 分割训练集和验证集\n",
    "\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# 定义数据处理函数\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# 对训练集和验证集进行编码和处理\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']),\n",
    "                                               torch.tensor(train_encodings['attention_mask']),\n",
    "                                               torch.tensor(train_labels))\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.tensor(val_encodings['input_ids']),\n",
    "                                             torch.tensor(val_encodings['attention_mask']),\n",
    "                                             torch.tensor(val_labels))\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained('uer/chinese_bert_wwm_ext', num_labels=3)\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=1000,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# 定义Trainer并进行训练\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=None,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=None,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 加载测试数据\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# 对测试数据进行编码和处理\n",
    "test_encodings = tokenizer(test_df['text'].tolist(), truncation=True, padding=True, max_length=128)\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']),\n",
    "                                              torch.tensor(test_encodings['attention_mask']),\n",
    "                                              torch.tensor(test_df['label'].tolist()))\n",
    "\n",
    "# 定义测试函数\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }\n",
    "\n",
    "# 进行测试并输出结果\n",
    "result = trainer.evaluate(test_dataset, metric_key_prefix='test')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
