{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    f=open('./data/test/text.txt', encoding='utf-8')\n",
    "    line = f.readline().strip() #读取第一行\n",
    "    sentence1 =[]\n",
    "    sentence1.append(line)\n",
    "    while line:  # 直到读取完文件\n",
    "        line = f.readline().strip()  # 读取一行文件，包括换行符\n",
    "        sentence1.append(line)\n",
    "    f.close()  # 关闭文件\n",
    "        \n",
    "    \n",
    "    f=open('./data/test/ymy.txt', encoding='utf-8')\n",
    "    line = f.readline().strip() #读取第一行\n",
    "    ymys1 =[]\n",
    "    ymys1.append(line)\n",
    "    while line:  # 直到读取完文件\n",
    "        line = f.readline().strip()  # 读取一行文件，包括换行符\n",
    "        ymys1.append(line)\n",
    "    f.close()  # 关闭文件\n",
    "    \n",
    "    f=open('./data/train/text.txt', encoding='utf-8')\n",
    "    line = f.readline().strip() #读取第一行\n",
    "    sentence =[]\n",
    "    sentence.append(line)\n",
    "    while line:  # 直到读取完文件\n",
    "        line = f.readline().strip()  # 读取一行文件，包括换行符\n",
    "        sentence.append(line)\n",
    "    f.close()  # 关闭文件\n",
    "        \n",
    "    \n",
    "    f=open('./data/train/ymy.txt', encoding='utf-8')\n",
    "    line = f.readline().strip() #读取第一行\n",
    "    ymys =[]\n",
    "    ymys.append(line)\n",
    "    while line:  # 直到读取完文件\n",
    "        line = f.readline().strip()  # 读取一行文件，包括换行符\n",
    "        ymys.append(line)\n",
    "    f.close()  # 关闭文件\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'医生：吃饭有没有改变啊'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1[7].replace(' ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1' not in ymys1[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = []\n",
    "for i in range(len(sentence1)):\n",
    "#     if '0' not in ymys1[i] and '1' not in ymys1[i] and '2' not in ymys1[i]:\n",
    "#         test_data.append([0,sentence1[i]])\n",
    "    if '0' in ymys1[i]:\n",
    "        valid_data.append([0,sentence1[i].replace(' ','')])\n",
    "    elif '1' in ymys1[i]:\n",
    "        valid_data.append([1,sentence1[i].replace(' ','')])\n",
    "    elif '2' in ymys1[i]:\n",
    "        valid_data.append([2,sentence1[i].replace(' ','')])\n",
    "        \n",
    "train_data = []\n",
    "for i in range(len(sentence1)):\n",
    "#     if '0' not in ymys1[i] and '1' not in ymys1[i] and '2' not in ymys1[i]:\n",
    "#         test_data.append([0,sentence1[i]])\n",
    "    if '0' in ymys[i]:\n",
    "        train_data.append([0,sentence[i].replace(' ','')])\n",
    "    elif '1' in ymys[i]:\n",
    "        train_data.append([1,sentence[i].replace(' ','')])\n",
    "    elif '2' in ymys[i]:\n",
    "        train_data.append([2,sentence[i].replace(' ','')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, '患者：没有，也没怎么哭闹'],\n",
       " [1, '医生：宝贝最近有没有呕吐症状呢？'],\n",
       " [1, '患者：呕吐，有时会吐，不多'],\n",
       " [1, '医生：宝贝如果呕吐量非常大的话就要给宝贝空肚子的奥，至少2小时不要喝水不要吃奶的'],\n",
       " [1, '医生：宝贝拉肚子到今天为止总共第几天了呢'],\n",
       " [1, '医生：宝贝到今天为止拉肚子总共几天了呢？'],\n",
       " [1, '医生：大便什么样子的呢？除了浠水之外，大便颜色正常吗？有没有粘液脓血之类呢？有没有未消化的奶瓣呢？'],\n",
       " [1, '患者：奶瓣有粘液有，颜色金黄'],\n",
       " [2,\n",
       "  '医生：宝贝目前的话大便的情况主要是一个消化不良的表现，消化不良的话可能受凉或者胃肠功能弱都可能引起来消化不良的，治疗的原则一般就是调节胃肠功能的治疗原则的奥'],\n",
       " [1,\n",
       "  '医生：宝贝目前的话大便次数还可以，大便有所减少，大便性状是奶瓣，浠水，颜色是正常的，目前的话精神状态好尿量多的话就问题不大的，病情也是比较稳定的，您给宝贝用了妈咪爱巩固大便有所好转，这个胃肠功能的话这个一般3到5天的疗程，再者益生菌吸收也是一个过程的，所以目前才用了两天，目前的病情又比较稳定，所以我建议可以继续给宝贝巩固点妈咪爱调节胃肠功能'],\n",
       " [2,\n",
       "  '医生：吃着益生菌观察两天，大便次数大便性状不缓解或者加重及时带宝贝去儿内科就诊查个大便常规明确一下病因，需不需要调整用药，根据结果对症处理，精神状态差，超过8小时没尿的话就可以及时带宝贝去儿内科就诊补液对症治疗了，防止宝贝脱水电解质紊乱的奥！'],\n",
       " [1, '医生：孩子拉肚子总共几天了呢？'],\n",
       " [0, '医生：孩子大便颜色呢？浠水便吗？大便有未消化的奶瓣吗？'],\n",
       " [0, '患者：没有奶瓣'],\n",
       " [1, '医生：孩子这个目前的症状主要是消化不良的表现，主要是由于着凉，饮食不当等因素引起来的，治疗原则主要是调节胃肠功能的治疗原则']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bert4keras.backend import keras, K\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.tokenizers import Tokenizer, load_vocab\n",
    "from bert4keras.optimizers import Adam\n",
    "from bert4keras.snippets import sequence_padding, DataGenerator\n",
    "from bert4keras.snippets import open\n",
    "from bert4keras.snippets import to_array\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm\n",
    "import fairies as fa\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "maxlen = 128\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 5e-4\n",
    "p = './bert_weight_file/uncased_L-4_H-128_A-2/'\n",
    "config_path = p + 'bert_config.json'\n",
    "checkpoint_path = p + 'bert_model.ckpt'\n",
    "dict_path = './bert_weight_file/Chinese-BERT-wwm/' + 'vocab.txt' #因为使用了中文词汇量大的字典，acc提高了\n",
    "tokenizer = Tokenizer(dict_path, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict, keep_tokens = load_vocab(\n",
    "    dict_path=dict_path,\n",
    "    simplified=True,\n",
    "    startswith=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator(DataGenerator):\n",
    "    def __iter__(self, random=False):\n",
    "\n",
    "        idxs = list(range(len(self.data)))\n",
    "        if random:\n",
    "            np.random.shuffle(idxs)\n",
    "        batch_token_ids, batch_segment_ids, batch_a_token_ids = [], [], []\n",
    "\n",
    "        for i in idxs:\n",
    "\n",
    "            data = self.data[i]\n",
    "            text = \"这句话表示判断症状状态为是\"\n",
    "            #text = \"两句话意思相同\"\n",
    "            text1 = data[1]\n",
    "            label = data[0]\n",
    "\n",
    "\n",
    "            final_text = text + ':' + text1 \n",
    "\n",
    "            token_ids, segment_ids = tokenizer.encode(\n",
    "                final_text, maxlen=maxlen)\n",
    "\n",
    "            # mask掉'相'字\n",
    "            token_ids[13] = tokenizer._token_mask_id\n",
    "            \n",
    "            \n",
    "\n",
    "            if label == 0:\n",
    "                a_token_ids, _ = tokenizer.encode('否')\n",
    "   \n",
    "            if label == 1:\n",
    "                a_token_ids, _ = tokenizer.encode('是')\n",
    "\n",
    "            if label == 2:\n",
    "                a_token_ids, _ = tokenizer.encode('像')\n",
    "\n",
    "            batch_token_ids.append(token_ids)\n",
    "            batch_segment_ids.append(segment_ids)\n",
    "            batch_a_token_ids.append(a_token_ids[1:])\n",
    "\n",
    "            if len(batch_token_ids) == self.batch_size or i == idxs[-1]:\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                batch_a_token_ids = sequence_padding(batch_a_token_ids, 1)\n",
    "                yield [batch_token_ids, batch_segment_ids], batch_a_token_ids\n",
    "                batch_token_ids, batch_segment_ids, batch_a_token_ids = [], [], []\n",
    "\n",
    "train_generator = data_generator(train_data, batch_size)\n",
    "valid_generator = data_generator(valid_data, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (Embedding)     multiple             1738880     Input-Token[0][0]                \n",
      "                                                                 MLM-Norm[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, None, 128)    0           Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    66048       Embedding-Dropout[0][0]          \n",
      "                                                                 Embedding-Dropout[0][0]          \n",
      "                                                                 Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    0           Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    0           Embedding-Dropout[0][0]          \n",
      "                                                                 Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 128)    256         Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward (Feed (None, None, 128)    131712      Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Dropo (None, None, 128)    0           Transformer-0-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Add ( (None, None, 128)    0           Transformer-0-MultiHeadSelfAttent\n",
      "                                                                 Transformer-0-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Norm  (None, None, 128)    256         Transformer-0-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    66048       Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    0           Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    0           Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 128)    256         Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward (Feed (None, None, 128)    131712      Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Dropo (None, None, 128)    0           Transformer-1-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Add ( (None, None, 128)    0           Transformer-1-MultiHeadSelfAttent\n",
      "                                                                 Transformer-1-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Norm  (None, None, 128)    256         Transformer-1-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 128)    66048       Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 128)    0           Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 128)    0           Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 128)    256         Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward (Feed (None, None, 128)    131712      Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Dropo (None, None, 128)    0           Transformer-2-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Add ( (None, None, 128)    0           Transformer-2-MultiHeadSelfAttent\n",
      "                                                                 Transformer-2-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Norm  (None, None, 128)    256         Transformer-2-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 128)    66048       Transformer-2-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 128)    0           Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 128)    0           Transformer-2-FeedForward-Norm[0]\n",
      "                                                                 Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 128)    256         Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward (Feed (None, None, 128)    131712      Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward-Dropo (None, None, 128)    0           Transformer-3-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward-Add ( (None, None, 128)    0           Transformer-3-MultiHeadSelfAttent\n",
      "                                                                 Transformer-3-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward-Norm  (None, None, 128)    256         Transformer-3-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Dense (Dense)               (None, None, 128)    16512       Transformer-3-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Norm (LayerNormalization)   (None, None, 128)    256         MLM-Dense[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "MLM-Bias (BiasAdd)              (None, None, 13585)  13585       Embedding-Token[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "MLM-Activation (Activation)     (None, None, 13585)  0           MLM-Bias[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, None, 13585)  0           MLM-Activation[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 2,628,369\n",
      "Trainable params: 2,628,369\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "34/34 [==============================] - 184s 5s/step - loss: 2.0731\n",
      "0.6514203177660086 1\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 178s 5s/step - loss: 0.8899\n",
      "0.6547905633124699 1\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 172s 5s/step - loss: 0.8387\n",
      "0.6716417910447762 1\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 166s 5s/step - loss: 0.7759\n",
      "0.6990852190659606 1\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 175s 5s/step - loss: 0.7345\n",
      "0.6769378911892152 1\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 170s 5s/step - loss: 0.6790\n",
      "0.7048627828598941 1\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 168s 5s/step - loss: 0.6589\n",
      "0.6822339913336543 1\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 170s 5s/step - loss: 0.6344\n",
      "0.7116032739528165 1\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 165s 5s/step - loss: 0.5663\n",
      "0.7005296100144439 1\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 164s 5s/step - loss: 0.5415\n",
      "0.6750120365912373 1\n",
      "0.6750120365912373\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "model = build_transformer_model(\n",
    "    config_path,\n",
    "    checkpoint_path,\n",
    "    with_mlm=True,\n",
    "    keep_tokens=keep_tokens,  \n",
    ")\n",
    "\n",
    "output = Lambda(lambda x: x[:, 13:14])(model.output)\n",
    "model = Model(model.input, output)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "def masked_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"交叉熵作为loss，并mask掉padding部分的预测\n",
    "    \"\"\"\n",
    "    y_true = K.reshape(y_true, [K.shape(y_true)[0], -1])\n",
    "    y_mask = K.cast(K.not_equal(y_true, 0), K.floatx())\n",
    "    cross_entropy = K.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    cross_entropy = K.sum(cross_entropy * y_mask) / K.sum(y_mask)\n",
    "    return cross_entropy\n",
    "\n",
    "\n",
    "model.compile(loss=masked_cross_entropy, optimizer=Adam(learning_rate))\n",
    "\n",
    "\n",
    "\n",
    "def predict(data):\n",
    "\n",
    "    text = \"这句话表示判断症状状态为是\"\n",
    "    text1 = data[1]\n",
    "    label = data[0]\n",
    "\n",
    "    final_text = text + ':' + text1\n",
    "    token_ids, segment_ids = tokenizer.encode(final_text, maxlen=maxlen)\n",
    "\n",
    "\n",
    "    # mask掉'相'字\n",
    "    token_ids[13] = tokenizer._token_mask_id\n",
    "    token_ids, segment_ids = to_array([token_ids], [segment_ids])\n",
    "\n",
    "    # 用mlm模型预测被mask掉的部分\n",
    "    probas = model.predict([token_ids, segment_ids])[0]\n",
    "    res = tokenizer.decode(probas.argmax(axis=1))\n",
    "\n",
    "    if res == '否' and label == 0:\n",
    "        return '正确' \n",
    "    elif res == '是' and label == 1:\n",
    "        return '正确'\n",
    "    elif res == '像' and label == 2:\n",
    "        return '正确'\n",
    "    elif res != '否' and res != '是' and res != '像':\n",
    "        return '超出范围'\n",
    "    else:\n",
    "        return '错误'\n",
    "\n",
    "\n",
    "def evaluat_vail_data(valid_data):\n",
    "\n",
    "    right, out, all = 1, 1, 1\n",
    "\n",
    "    for valid in valid_data:\n",
    "        res = predict(valid)\n",
    "        \n",
    "\n",
    "        if res == '正确':\n",
    "            right += 1\n",
    "        elif res == '超出范围':\n",
    "            out += 1\n",
    "\n",
    "        all += 1\n",
    "\n",
    "    return right / all, out\n",
    "\n",
    "\n",
    "class Evaluator(keras.callbacks.Callback):\n",
    "    \"\"\"评估与保存\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.lowest = 1e10\n",
    "        self.best = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # 保存最优\n",
    "        # if logs['loss'] <= self.lowest:\n",
    "        #     self.lowest = logs['loss']\n",
    "        #     model.save_weights('./best_model.weights')\n",
    "        acc, out = evaluat_vail_data(valid_data)\n",
    "        print(acc, out)\n",
    "        if acc >= self.best:\n",
    "            self.best = acc\n",
    "            model.save_weights('./best_model.weights')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    evaluator = Evaluator()\n",
    "    train_generator = data_generator(train_data, batch_size)\n",
    "    valid_generator = data_generator(valid_data, batch_size)\n",
    "    \n",
    "    def scheduler(epoch):\n",
    "        return learning_rate/(max(2*(epochs-1),1))\n",
    "\n",
    "    lr_scheduler = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "    \n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        train_generator.forfit(),\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        callbacks=[evaluator,lr_scheduler])\n",
    "\n",
    "    model.save_weights('./best_model.weights')\n",
    "    acc, out = evaluat_vail_data(valid_data)\n",
    "    print(acc)\n",
    "    print(out)\n",
    "    # 0.8981527708437343\n",
    "    # 0.9041437843235147\n",
    "else:\n",
    "    model.load_weights('./best_model.weights')\n",
    "    evaluat_vail_data(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
