{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    f=open('./data/test/text.txt', encoding='utf-8')\n",
    "    line = f.readline().strip() #读取第一行\n",
    "    sentence1 =[]\n",
    "    sentence1.append(line)\n",
    "    while line:  # 直到读取完文件\n",
    "        line = f.readline().strip()  # 读取一行文件，包括换行符\n",
    "        sentence1.append(line)\n",
    "    f.close()  # 关闭文件\n",
    "        \n",
    "    \n",
    "    f=open('./data/test/ymy.txt', encoding='utf-8')\n",
    "    line = f.readline().strip() #读取第一行\n",
    "    ymys1 =[]\n",
    "    ymys1.append(line)\n",
    "    while line:  # 直到读取完文件\n",
    "        line = f.readline().strip()  # 读取一行文件，包括换行符\n",
    "        ymys1.append(line)\n",
    "    f.close()  # 关闭文件\n",
    "    \n",
    "    f=open('./data/train/text.txt', encoding='utf-8')\n",
    "    line = f.readline().strip() #读取第一行\n",
    "    sentence =[]\n",
    "    sentence.append(line)\n",
    "    while line:  # 直到读取完文件\n",
    "        line = f.readline().strip()  # 读取一行文件，包括换行符\n",
    "        sentence.append(line)\n",
    "    f.close()  # 关闭文件\n",
    "        \n",
    "    \n",
    "    f=open('./data/train/ymy.txt', encoding='utf-8')\n",
    "    line = f.readline().strip() #读取第一行\n",
    "    ymys =[]\n",
    "    ymys.append(line)\n",
    "    while line:  # 直到读取完文件\n",
    "        line = f.readline().strip()  # 读取一行文件，包括换行符\n",
    "        ymys.append(line)\n",
    "    f.close()  # 关闭文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = []\n",
    "for i in range(len(sentence1)):\n",
    "#     if '0' not in ymys1[i] and '1' not in ymys1[i] and '2' not in ymys1[i]:\n",
    "#         test_data.append([0,sentence1[i]])\n",
    "    if '0' in ymys1[i]:\n",
    "        valid_data.append([0,sentence1[i].replace(' ','')])\n",
    "    elif '1' in ymys1[i]:\n",
    "        valid_data.append([1,sentence1[i].replace(' ','')])\n",
    "    elif '2' in ymys1[i]:\n",
    "        valid_data.append([2,sentence1[i].replace(' ','')])\n",
    "        \n",
    "train_data = []\n",
    "for i in range(len(sentence1)):\n",
    "#     if '0' not in ymys1[i] and '1' not in ymys1[i] and '2' not in ymys1[i]:\n",
    "#         test_data.append([0,sentence1[i]])\n",
    "    if '0' in ymys[i]:\n",
    "        train_data.append([0,sentence[i].replace(' ','')])\n",
    "    elif '1' in ymys[i]:\n",
    "        train_data.append([1,sentence[i].replace(' ','')])\n",
    "    elif '2' in ymys[i]:\n",
    "        train_data.append([2,sentence[i].replace(' ','')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './text_data/train.txt'\n",
    "with open(filename,'w',encoding='utf-8') as f:\n",
    "    for i in range(len(train_data)):\n",
    "        f.write(train_data[i][1]+ '\\t'+ str(train_data[i][0]) +'\\n')\n",
    "\n",
    "                \n",
    "filename = './text_data/test.txt'\n",
    "with open(filename,'w',encoding='utf-8') as f:\n",
    "    for i in range(len(valid_data)):\n",
    "        f.write(valid_data[i][1]+ '\\t'+ str(valid_data[i][0]) +'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "sents = [\n",
    "    '选择珠江花园的原因就是方便。',\n",
    "    '笔记本的键盘确实爽。',\n",
    "    '房间太小。其他的都一般。',\n",
    "    '今天才知道这书还有第6卷,真有点郁闷.',\n",
    "    '机器背面似乎被撕了张什么标签，残胶还在。',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tokenizer.batch_encode_plus(\n",
    "    batch_text_or_text_pairs=[sents[0], sents[1]],\n",
    "    \n",
    "    # 添加特殊符号\n",
    "    add_special_tokens=True,\n",
    "\n",
    "    # 当句子长度大于max_length时,截断\n",
    "    truncation=True,\n",
    "\n",
    "    # 一律补零到max_length长度\n",
    "    padding='max_length',\n",
    "    max_length=15,\n",
    "\n",
    "    # 可取值tf,pt,np,默认为返回list\n",
    "    return_tensors=None,\n",
    "\n",
    "    # 返回token_type_ids句子分段标识\n",
    "    return_token_type_ids=True,\n",
    "\n",
    "    # 返回attention_mask [PAD]补全标识\n",
    "    return_attention_mask=True,\n",
    "\n",
    "    # 返回special_tokens_mask 特殊符号标识\n",
    "    return_special_tokens_mask=True,\n",
    "\n",
    "    # 返回length 标识长度\n",
    "    return_length=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [[101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 102], [101, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0]]\n",
      "token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]\n",
      "special_tokens_mask : [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]]\n",
      "length : [15, 15]\n"
     ]
    }
   ],
   "source": [
    "for k, v in out.items():\n",
    "    print(k, ':', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "pretrained = AutoModel.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_text_len():\n",
    "    text_len = []\n",
    "    with open('./text_data/train.txt', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            text, _ = line.split('\\t')\n",
    "            text_len.append(len(text))\n",
    "    plt.hist(text_len)\n",
    "    plt.show()\n",
    "    print(max(text_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n"
     ]
    }
   ],
   "source": [
    "count_text_len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "# 模型保存位置\n",
    "MODEL_DIR = './text_data/models/'\n",
    "\n",
    "# 类别标签位置\n",
    "LABEL_PATH = './data/input/class.txt'\n",
    "\n",
    "# max_length\n",
    "TEXT_LEN = 210\n",
    "\n",
    "# TextCNN 模型参数\n",
    "EMBEDDING_DIM = 768\n",
    "NUM_FILTERS = 256\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# 训练参数\n",
    "EPOCH = 1\n",
    "LR = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split='train'):\n",
    "        super(Dataset, self).__init__()\n",
    "        path = \"./text_data/\" + str(split) + \".txt\"\n",
    "        self.lines = open(path, encoding='utf-8').readlines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text, label = self.lines[i].strip().split('\\t')\n",
    "        return text, int(label)\n",
    "def collate_fn(data):\n",
    "    sents = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    data = tokenizer.batch_encode_plus(\n",
    "        sents,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=TEXT_LEN,\n",
    "        return_tensors=\"pt\",\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    input_ids = data[\"input_ids\"]\n",
    "    attention_mask = data[\"attention_mask\"]\n",
    "    # token_type_ids = data[\"token_type_ids\"]  # 只有一个句子，不需要传这个参数\n",
    "    labels = torch.LongTensor(labels)\n",
    "    return input_ids, attention_mask, labels\n",
    "\n",
    "def get_label():\n",
    "    text = open(LABEL_PATH).read()\n",
    "    id2label = text.split()\n",
    "    return id2label, {v: k for k, v in enumerate(id2label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(1, NUM_FILTERS, (2, EMBEDDING_DIM))\n",
    "        self.conv2 = torch.nn.Conv2d(1, NUM_FILTERS, (3, EMBEDDING_DIM))\n",
    "        self.conv3 = torch.nn.Conv2d(1, NUM_FILTERS, (4, EMBEDDING_DIM))\n",
    "        self.linear = torch.nn.Linear(NUM_FILTERS * 3, NUM_CLASSES)\n",
    "\n",
    "    def conv_and_pool(self, conv, input):\n",
    "        out = conv(input)\n",
    "        out = F.relu(out)\n",
    "        return F.max_pool2d(out, (out.shape[2], out.shape[3])).squeeze()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)[0].unsqueeze(1)\n",
    "        out1 = self.conv_and_pool(self.conv1, out)\n",
    "        out2 = self.conv_and_pool(self.conv2, out)\n",
    "        out3 = self.conv_and_pool(self.conv3, out)\n",
    "        out = torch.cat([out1, out2, out3], dim=1)\n",
    "        return self.linear(out).softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_dataset = Dataset('train')\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "    dev_dataset = Dataset('test')\n",
    "    dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=32, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "    model = TextCNN()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    dev_acc_list = []\n",
    "\n",
    "    model.train()\n",
    "    for e in range(EPOCH):\n",
    "        for i, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
    "            out = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if i % 5 == 0:\n",
    "                out = out.argmax(dim=1)\n",
    "                acc = (out == labels).sum().item() / len(labels)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    input_ids_, attention_mask_, labels_ = iter(dev_loader).__next__()\n",
    "                    out_ = model(input_ids_, attention_mask_)\n",
    "                    out_ = out_.argmax(dim=1)\n",
    "                    dev_acc = (out_ == labels_).sum().item() / len(labels_)\n",
    "                if dev_acc_list != [] and dev_acc >= max(dev_acc_list):\n",
    "                    torch.save(model, MODEL_DIR + f'{e}.pth')\n",
    "                    print('save model success')\n",
    "                dev_acc_list.append(dev_acc)\n",
    "\n",
    "                print(\n",
    "                    ' epoch: ', e,\n",
    "                    ' batch: ', i,\n",
    "                    ' loss: ', round(loss.item(), 2),\n",
    "                    ' train_acc: ', acc,\n",
    "                    ' dev_acc: ', dev_acc,\n",
    "                )\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    test_dataset = Dataset('test')\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, collate_fn=collate_fn, batch_size=64, shuffle=False)\n",
    "\n",
    "    model = torch.load(MODEL_DIR + '0.pth')\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (input_ids, attention_mask, labels) in enumerate(test_loader):\n",
    "            out = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(out, labels)\n",
    "\n",
    "            out = out.argmax(dim=1)\n",
    "            correct += (out == labels).sum().item()\n",
    "            total += len(labels)\n",
    "\n",
    "            print(' batch: ', i, ' loss ', loss.item())\n",
    "\n",
    "    acc = correct / total\n",
    "    print(' acc: ', round(acc, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:  0  batch:  0  loss:  2.3  train_acc:  0.0  dev_acc:  0.0\n",
      " epoch:  0  batch:  5  loss:  2.21  train_acc:  0.5625  dev_acc:  0.65625\n",
      " epoch:  0  batch:  10  loss:  2.14  train_acc:  0.59375  dev_acc:  0.59375\n",
      " epoch:  0  batch:  15  loss:  2.01  train_acc:  0.65625  dev_acc:  0.625\n",
      " epoch:  0  batch:  20  loss:  2.03  train_acc:  0.5  dev_acc:  0.65625\n",
      " epoch:  0  batch:  25  loss:  1.95  train_acc:  0.5625  dev_acc:  0.5625\n",
      " epoch:  0  batch:  30  loss:  1.9  train_acc:  0.59375  dev_acc:  0.8125\n",
      " epoch:  0  batch:  35  loss:  1.86  train_acc:  0.625  dev_acc:  0.71875\n",
      " epoch:  0  batch:  40  loss:  1.85  train_acc:  0.625  dev_acc:  0.6875\n",
      " epoch:  0  batch:  45  loss:  1.73  train_acc:  0.75  dev_acc:  0.78125\n",
      " epoch:  0  batch:  50  loss:  1.69  train_acc:  0.78125  dev_acc:  0.625\n",
      " epoch:  0  batch:  55  loss:  1.75  train_acc:  0.71875  dev_acc:  0.78125\n",
      " epoch:  0  batch:  60  loss:  1.78  train_acc:  0.6875  dev_acc:  0.65625\n",
      " epoch:  0  batch:  65  loss:  1.68  train_acc:  0.78125  dev_acc:  0.65625\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch:  0  loss  1.8751140832901\n",
      " batch:  1  loss  1.8140372037887573\n",
      " batch:  2  loss  1.9177448749542236\n",
      " batch:  3  loss  1.950440764427185\n",
      " batch:  4  loss  1.844298243522644\n",
      " batch:  5  loss  1.890299916267395\n",
      " batch:  6  loss  1.7748558521270752\n",
      " batch:  7  loss  1.9101195335388184\n",
      " batch:  8  loss  1.9189207553863525\n",
      " batch:  9  loss  1.8886750936508179\n",
      " batch:  10  loss  1.8255690336227417\n",
      " batch:  11  loss  1.8009395599365234\n",
      " batch:  12  loss  1.8154155015945435\n",
      " batch:  13  loss  1.7090814113616943\n",
      " batch:  14  loss  1.7670254707336426\n",
      " batch:  15  loss  1.9153435230255127\n",
      " batch:  16  loss  1.9195125102996826\n",
      " batch:  17  loss  1.7774369716644287\n",
      " batch:  18  loss  1.8560340404510498\n",
      " batch:  19  loss  1.80027174949646\n",
      " batch:  20  loss  1.9347952604293823\n",
      " batch:  21  loss  1.7571898698806763\n",
      " batch:  22  loss  1.8328220844268799\n",
      " batch:  23  loss  1.800118088722229\n",
      " batch:  24  loss  1.8864378929138184\n",
      " batch:  25  loss  1.8599448204040527\n",
      " batch:  26  loss  1.6992714405059814\n",
      " batch:  27  loss  1.7453598976135254\n",
      " batch:  28  loss  1.7755228281021118\n",
      " batch:  29  loss  1.7743511199951172\n",
      " batch:  30  loss  1.916536808013916\n",
      " batch:  31  loss  1.7866528034210205\n",
      " batch:  32  loss  1.8120317459106445\n",
      " acc:  0.65\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
