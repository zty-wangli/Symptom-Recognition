{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf \n",
    "from bert4keras.backend import K,keras,search_layer\n",
    "from bert4keras.snippets import ViterbiDecoder,to_array\n",
    "\n",
    "from data_load import *\n",
    "from build_model import bert_bilstm_crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# 固定随机种子\n",
    "seed = 233\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHSHSEED'] = str(seed)\n",
    "\n",
    "# 权重参数\n",
    "epochs = 4\n",
    "batch_size = 16\n",
    "lstm_units = 128\n",
    "drop_rate = 0.1 #有改动0.1-》0.01\n",
    "learning_rate = 5e-5\n",
    "max_len =168\n",
    "\n",
    "\n",
    "\n",
    "# 权重路径\n",
    "config_path = './bert_weight_file/uncased_L-4_H-768_A-12/bert_config.json'\n",
    "checkpoint_path = './bert_weight_file/uncased_L-4_H-768_A-12/bert_model.ckpt'\n",
    "\n",
    "# 模型保存路径\n",
    "model_save_path = './save_model/bert_bilstm_crf.weight'\n",
    "CRF_save_path = './save_model/CRF.npy'\n",
    "\n",
    "class NamedEntityRecognizer(ViterbiDecoder):\n",
    "    \"\"\"命名实体识别器\n",
    "    \"\"\"\n",
    "    def recognize(self, text):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        while len(tokens) > max_len:\n",
    "            tokens.pop(-2)\n",
    "        mapping = tokenizer.rematch(text, tokens)\n",
    "        token_ids = tokenizer.tokens_to_ids(tokens)\n",
    "        segment_ids = [0] * len(token_ids)\n",
    "        token_ids, segment_ids = to_array([token_ids], [segment_ids]) # ndarray\n",
    "        nodes = model.predict([token_ids, segment_ids])[0] # [sqe_len,23]\n",
    "        labels = self.decode(nodes) # id [sqe_len,], [0 0 0 0 0 7 8 8 0 0 0 0 0 0 0]\n",
    "        entities, starting = [], False\n",
    "        for i, label in enumerate(labels):\n",
    "            if label > 0:\n",
    "                if label % 2 == 1:\n",
    "                    starting = True\n",
    "                    entities.append([[i], id2label[(label - 1) // 2]])\n",
    "                elif starting:\n",
    "                    entities[-1][0].append(i)\n",
    "                else:\n",
    "                    starting = False\n",
    "            else:\n",
    "                starting = False\n",
    "        return [(text[mapping[w[0]][0]:mapping[w[-1]][-1] + 1], l) for w, l in entities]\n",
    "    \n",
    "#相等应加set（）中源文本的数量    \n",
    "def ner_metrics(data):\n",
    "    X,Y,Z = 1e-6,1e-6,1e-6\n",
    "    count = 0\n",
    "    for d in tqdm(data):\n",
    "        text = ''.join([i[0] for i in d])\n",
    "        pred= NER.recognize(text)\n",
    "        R = set(pred)\n",
    "        T = set([tuple(i) for i in d if i[1] != 'O'])\n",
    "        \n",
    "        # 便于T和R做交集\n",
    "        m = []\n",
    "        for i in T:\n",
    "            for j in i[0]:\n",
    "                m.append((j,i[1]))\n",
    "        T = set(m)\n",
    "\n",
    "        X += len(R&T)\n",
    "        Y += len(R)\n",
    "        Z += len(T)\n",
    "        count += 1\n",
    "\n",
    "    f1,precision,recall = 2 * X / (Y + Z),X / Y,X / Z\n",
    "    return f1,precision,recall\n",
    "\n",
    "class Evaluator(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(Evaluator, self).__init__()\n",
    "        self.best_val_f1 = 0\n",
    "    def on_epoch_end(self, epoch,logs=None):\n",
    "        NER.trans = K.eval(CRF.trans) # 可能有错\n",
    "        f1, precision, recall = ner_metrics(valid_data)\n",
    "        if f1 > self.best_val_f1:\n",
    "            model.save_weights(model_save_path)\n",
    "            self.best_val_f1 = f1\n",
    "            print('save model to {}'.format(checkpoint_path))\n",
    "        else:\n",
    "            global learning_rate\n",
    "            learning_rate = learning_rate / 5\n",
    "        print(\n",
    "              'valid: f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f\\n' %\n",
    "              (f1,precision,recall,self.best_val_f1)\n",
    "        )\n",
    "        \n",
    "# def adversarial_training(model, embedding_name, epsilon=1):\n",
    "#     \"\"\"\n",
    "#     给模型添加对抗训练\n",
    "#     其中model是需要添加对抗训练的keras模型\n",
    "#     \"\"\"\n",
    "#     if model.train_function is None:  # 如果还没有训练函数\n",
    "#         model._make_train_function()  # 手动make\n",
    "#     old_train_function = model.train_function  # 备份旧的训练函数\n",
    "\n",
    "#     # 查找Embedding层\n",
    "#     for output in model.outputs:\n",
    "#         embedding_layer = search_layer(output, embedding_name)\n",
    "#         if embedding_layer is not None:\n",
    "#             break\n",
    "#     if embedding_layer is None:\n",
    "#         raise Exception('Embedding layer not found')\n",
    "\n",
    "#     # 求Embedding梯度\n",
    "#     embeddings = embedding_layer.embeddings  # Embedding矩阵\n",
    "#     gradients = K.gradients(model.total_loss, [embeddings])  # Embedding梯度\n",
    "#     gradients = K.zeros_like(embeddings) + gradients[0]  # 转为dense tensor\n",
    "\n",
    "#     # 封装为函数\n",
    "#     inputs = (\n",
    "#         model._feed_inputs + model._feed_targets + model._feed_sample_weights\n",
    "#     )  # 所有输入层\n",
    "#     embedding_gradients = K.function(\n",
    "#         inputs=inputs,\n",
    "#         outputs=[gradients],\n",
    "#         name='embedding_gradients',\n",
    "#     )  # 封装为函数\n",
    "\n",
    "#     def train_function(inputs):\n",
    "#         # 重新定义训练函数\n",
    "#         grads = embedding_gradients(inputs)[0]  # Embedding梯度\n",
    "#         delta = epsilon * grads / (np.sqrt((grads**2).sum()) + 1e-8)  # 计算扰动\n",
    "#         K.set_value(embeddings, K.eval(embeddings) + delta)  # 注入扰动\n",
    "#         outputs = old_train_function(inputs)  # 梯度下降\n",
    "#         K.set_value(embeddings, K.eval(embeddings) - delta)  # 删除扰动\n",
    "#         return outputs\n",
    "#     model.train_function = train_function  # 覆盖原训练函数        \n",
    "\n",
    "\n",
    "\n",
    "model,CRF = bert_bilstm_crf(config_path,checkpoint_path,num_labels,lstm_units,drop_rate,learning_rate)\n",
    "# adversarial_training(model,'Embedding-Token',0.5)\n",
    "NER = NamedEntityRecognizer(trans=K.eval(CRF.trans), starts=[0], ends=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/4\n",
      "3718/3718 [==============================] - 8400s 2s/step - loss: 0.2471 - sparse_accuracy: 0.9956 - val_loss: 1.5426 - val_sparse_accuracy: 0.9762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [03:28<00:00, 39.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model to ./bert_weight_file/uncased_L-4_H-768_A-12/bert_model.ckpt\n",
      "valid: f1: 0.89317, precision: 0.84999, recall: 0.94098, best f1: 0.89317\n",
      "\n",
      "Epoch 2/4\n",
      "3718/3718 [==============================] - 8460s 2s/step - loss: 0.1783 - sparse_accuracy: 0.9967 - val_loss: 0.3871 - val_sparse_accuracy: 0.9747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [03:26<00:00, 40.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model to ./bert_weight_file/uncased_L-4_H-768_A-12/bert_model.ckpt\n",
      "valid: f1: 0.89405, precision: 0.84702, recall: 0.94662, best f1: 0.89405\n",
      "\n",
      "Epoch 3/4\n",
      "3718/3718 [==============================] - 8517s 2s/step - loss: 0.0806 - sparse_accuracy: 0.9985 - val_loss: 0.3699 - val_sparse_accuracy: 0.9802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [03:29<00:00, 39.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model to ./bert_weight_file/uncased_L-4_H-768_A-12/bert_model.ckpt\n",
      "valid: f1: 0.90605, precision: 0.86587, recall: 0.95014, best f1: 0.90605\n",
      "\n",
      "Epoch 4/4\n",
      "3718/3718 [==============================] - 8523s 2s/step - loss: 0.0360 - sparse_accuracy: 0.9993 - val_loss: 2.3349 - val_sparse_accuracy: 0.9791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [03:28<00:00, 39.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: f1: 0.90277, precision: 0.85609, recall: 0.95482, best f1: 0.90605\n",
      "\n",
      "[[ 0.5732396  -0.6188147  -0.12123274  0.10713068 -0.4679119  -0.59594375\n",
      "  -0.5330648  -0.3448834  -0.1026888  -0.68678284 -0.38008773]\n",
      " [-0.3268835  -0.22179076 -0.41482008  0.20347288  0.15495309 -0.54423153\n",
      "  -0.35120732 -0.22765681  0.2145274  -0.02894322 -0.12005794]\n",
      " [-0.6355784  -0.2859531  -0.1884572  -0.39584544 -0.36548477 -0.37046388\n",
      "   0.44329137 -0.0971401  -0.4965401  -0.18640006 -0.17739114]\n",
      " [-0.6253033   0.03999732  0.18687652 -0.00191786 -0.07030854 -0.00507968\n",
      "   0.14093065 -0.01452608 -0.30369285 -0.6493261  -0.17835732]\n",
      " [-0.6931408  -0.26227516  0.36901703 -0.23902875  0.06996739 -0.5383688\n",
      "  -0.5162194   0.07795113  0.47598404 -0.44287404  0.50213104]\n",
      " [ 0.29303068  0.33775932  0.0663694   0.1427771  -0.19603327 -0.288441\n",
      "  -0.38806555 -0.50234056 -0.6440322  -0.02040486 -0.3626405 ]\n",
      " [-0.81960636  0.03804528  0.25959396 -0.3833121   0.34006646 -0.26488525\n",
      "  -0.02490926 -0.11169314  0.22222681 -0.79255384 -0.10842375]\n",
      " [-0.24861024 -0.09059444  0.36263838  0.21271582 -0.5397169   0.2119512\n",
      "  -0.12238833  0.36842227 -0.33649278  0.05268805 -0.2761031 ]\n",
      " [ 0.09000506 -0.3764084  -0.30264416 -0.4432797  -0.5048228  -0.5881529\n",
      "  -0.18346187 -0.43011504  0.31205902 -0.00252945  0.00969012]\n",
      " [-0.58588463  0.21310402 -0.6469165  -0.41246176 -0.54252106 -0.09583908\n",
      "  -0.2589223  -0.22959615 -0.03285508  0.55363166 -0.74456435]\n",
      " [-0.20044358  0.1568645   0.28493223  0.28683627 -0.48321387 -0.65134436\n",
      "   0.28820124  0.27278313 -0.16818567  0.00982556  0.15912151]]\n",
      "(11, 11)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_data,_ = load_data('./ner_data/train/train.txt',128)\n",
    "    valid_data,_ = load_data('./ner_data/dev/test.txt',128)\n",
    "    \n",
    "    flag = False\n",
    "    count = 0\n",
    "    i = 0\n",
    "    while(i<len(train_data)):\n",
    "        if flag==True:\n",
    "            i = i-1\n",
    "        if train_data[i][0][1] == 'O'and len(train_data[i])==1:\n",
    "            del train_data[i]\n",
    "            flag = True\n",
    "            count+=1\n",
    "        else:\n",
    "            for j in range(count):\n",
    "                train_data.append(train_data[i])\n",
    "            flag = False\n",
    "            count = 0\n",
    "        i += 1\n",
    "    \n",
    "    train_generator = data_generator(train_data, batch_size)\n",
    "    valid_generator = data_generator(valid_data, batch_size*5)\n",
    "    \n",
    "    evaluator = Evaluator()\n",
    "    \n",
    "    def scheduler(epoch):\n",
    "        return learning_rate/(max(2*(epoch-1),1))\n",
    "\n",
    "    lr_scheduler = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    \n",
    "    model.fit(\n",
    "        train_generator.forfit(),\n",
    "        steps_per_epoch = len(train_generator),\n",
    "        validation_data = valid_generator.forfit(),\n",
    "        validation_steps = len(valid_generator),\n",
    "        epochs = epochs,\n",
    "        callbacks = [evaluator,lr_scheduler]\n",
    "    )\n",
    "    \n",
    "    print(K.eval(CRF.trans))\n",
    "    print(K.eval(CRF.trans).shape)\n",
    "    model.save_weights(model_save_path)\n",
    "    np.save(CRF_save_path, K.eval(CRF.trans))\n",
    "\n",
    "    # torch.save(model, model_save_path)\n",
    "    # pickle.dump(K.eval(CRF.trans),open('./save_model/crf_trans.pkl','rb'))\n",
    "    \n",
    "else:\n",
    "    # model = torch.load(model_save_path)\n",
    "    model.load_weights(model_save_path)\n",
    "    # NER.trans = pickle.load(open('./save_model/crf_trans.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/4\n",
      "3718/3718 [==============================] - 7165s 2s/step - loss: 0.2264 - sparse_accuracy: 0.9937 - val_loss: 0.5084 - val_sparse_accuracy: 0.9859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [03:37<00:00, 37.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model to ./bert_weight_file/uncased_L-4_H-768_A-12/bert_model.ckpt\n",
      "valid: f1: 0.92760, precision: 0.92177, recall: 0.93351, best f1: 0.92760\n",
      "\n",
      "Epoch 2/4\n",
      "3718/3718 [==============================] - 7260s 2s/step - loss: 0.1658 - sparse_accuracy: 0.9951 - val_loss: 0.2227 - val_sparse_accuracy: 0.9863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [03:57<00:00, 34.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: f1: 0.92492, precision: 0.91990, recall: 0.93000, best f1: 0.92760\n",
      "\n",
      "Epoch 3/4\n",
      "3718/3718 [==============================] - 7793s 2s/step - loss: 0.1397 - sparse_accuracy: 0.9956 - val_loss: 0.1660 - val_sparse_accuracy: 0.9862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [04:10<00:00, 33.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: f1: 0.92755, precision: 0.91609, recall: 0.93930, best f1: 0.92760\n",
      "\n",
      "Epoch 4/4\n",
      "3718/3718 [==============================] - 7804s 2s/step - loss: 0.1169 - sparse_accuracy: 0.9961 - val_loss: 1.1356 - val_sparse_accuracy: 0.9867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [04:17<00:00, 32.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model to ./bert_weight_file/uncased_L-4_H-768_A-12/bert_model.ckpt\n",
      "valid: f1: 0.93006, precision: 0.92065, recall: 0.93966, best f1: 0.93006\n",
      "\n",
      "[[ 6.61567867e-01 -7.68517792e-01 -7.32621372e-01 -2.85716578e-02\n",
      "  -1.07572722e+00 -7.15510130e-01 -1.13239563e+00 -4.00296718e-01\n",
      "  -7.17471063e-01 -8.86213601e-01 -9.88571763e-01]\n",
      " [-4.85753953e-01 -1.33429423e-01 -7.60351479e-01  9.17750224e-02\n",
      "  -1.94741100e-01 -6.42317533e-01 -6.96268857e-01 -2.71289468e-01\n",
      "  -1.38125718e-01 -1.16618574e-01 -4.69828427e-01]\n",
      " [-1.24130547e+00 -6.43295884e-01 -1.94629222e-01 -6.59347475e-01\n",
      "  -3.70389432e-01 -7.25305676e-01  4.33107167e-01 -2.23937005e-01\n",
      "  -5.03868937e-01 -6.83418512e-01 -1.86347455e-01]\n",
      " [-7.49212861e-01 -8.28977600e-02 -1.15238063e-01  7.61296898e-02\n",
      "  -3.69333923e-01 -1.07453898e-01 -1.55804187e-01 -5.58670275e-02\n",
      "  -6.08527839e-01 -7.31610000e-01 -4.79127169e-01]\n",
      " [-1.29739940e+00 -6.18033767e-01  3.60542953e-01 -4.98364419e-01\n",
      "   6.39630184e-02 -8.86259615e-01 -5.19447684e-01 -4.95445989e-02\n",
      "   4.61085826e-01 -9.29463983e-01  4.88358259e-01]\n",
      " [ 1.67916670e-01  2.53186911e-01 -2.50632107e-01  6.08023293e-02\n",
      "  -5.12790680e-01 -1.88391834e-01 -6.91450357e-01 -5.80010831e-01\n",
      "  -9.85112131e-01 -1.12636156e-01 -6.97279036e-01]\n",
      " [-1.42292213e+00 -3.16534072e-01  2.52324343e-01 -6.37688935e-01\n",
      "   3.32640141e-01 -6.16388500e-01 -2.98443399e-02 -2.34043285e-01\n",
      "   2.11012572e-01 -1.27232289e+00 -1.15773074e-01]\n",
      " [-2.95950592e-01 -1.68634057e-01  2.38633215e-01  1.82787657e-01\n",
      "  -6.49062812e-01  1.53579056e-01 -2.36175895e-01  4.00343359e-01\n",
      "  -4.63257372e-01 -3.13174026e-03 -3.95623803e-01]\n",
      " [-5.22074580e-01 -7.36377656e-01 -3.08298081e-01 -6.98249698e-01\n",
      "  -5.09201944e-01 -9.47490931e-01 -1.89162910e-01 -5.58562160e-01\n",
      "   2.95870721e-01 -5.02973437e-01 -1.29163102e-03]\n",
      " [-7.83718288e-01  1.04805589e-01 -1.12693012e+00 -4.88309115e-01\n",
      "  -1.02468336e+00 -2.01903194e-01 -7.39596128e-01 -2.91632384e-01\n",
      "  -5.37481129e-01  6.53244436e-01 -1.23164821e+00]\n",
      " [-8.09636950e-01 -2.04072937e-01  2.77136981e-01  2.23070271e-02\n",
      "  -4.86698627e-01 -1.00370288e+00  2.81153888e-01  1.41448453e-01\n",
      "  -1.76238522e-01 -4.87345248e-01  1.49195641e-01]]\n",
      "(11, 11)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epochs = 4\n",
    "    \n",
    "    train_data,_ = load_data('./ner_data/train/train.txt',128)\n",
    "    valid_data,_ = load_data('./ner_data/dev/test.txt',128)\n",
    "    \n",
    "\n",
    "    train_generator = data_generator(train_data, batch_size)\n",
    "    valid_generator = data_generator(valid_data, batch_size*5)\n",
    "    \n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        model_save_path,\n",
    "        monitor = 'val_sparse_accuracy',\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        mode = 'max'\n",
    "    )\n",
    "    evaluator = Evaluator()\n",
    "    \n",
    "#     def scheduler(epoch):\n",
    "#         return learning_rate/(max(2*(epoch-1),1))\n",
    "\n",
    "#     lr_scheduler = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    \n",
    "    model.fit(\n",
    "        train_generator.forfit(),\n",
    "        steps_per_epoch = len(train_generator),\n",
    "        validation_data = valid_generator.forfit(),\n",
    "        validation_steps = len(valid_generator),\n",
    "        epochs = epochs,\n",
    "        callbacks = [evaluator]\n",
    "    )\n",
    "    \n",
    "    print(K.eval(CRF.trans))\n",
    "    print(K.eval(CRF.trans).shape)\n",
    "    model.save_weights(model_save_path)\n",
    "    np.save(CRF_save_path, K.eval(CRF.trans))\n",
    "\n",
    "    # torch.save(model, model_save_path)\n",
    "    # pickle.dump(K.eval(CRF.trans),open('./save_model/crf_trans.pkl','rb'))\n",
    "    \n",
    "else:\n",
    "    # model = torch.load(model_save_path)\n",
    "    model.load_weights(model_save_path)\n",
    "    # NER.trans = pickle.load(open('./save_model/crf_trans.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
