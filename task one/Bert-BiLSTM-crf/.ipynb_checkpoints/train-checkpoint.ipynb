{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf \n",
    "from bert4keras.backend import K,keras,search_layer\n",
    "from bert4keras.snippets import ViterbiDecoder,to_array\n",
    "\n",
    "from data_load import *\n",
    "from build_model import bert_bilstm_crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# 固定随机种子\n",
    "seed = 233\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHSHSEED'] = str(seed)\n",
    "\n",
    "# 权重参数\n",
    "epochs = 4\n",
    "batch_size = 16\n",
    "lstm_units = 128\n",
    "drop_rate = 0.1 #有改动0.1-》0.01\n",
    "learning_rate = 5e-5\n",
    "max_len =168\n",
    "\n",
    "#精细训练\n",
    "fine_train_list = [0 for i in range(8275)]\n",
    "train_predict_list = []\n",
    "\n",
    "# 权重路径\n",
    "config_path = './bert_weight_file/uncased_L-4_H-768_A-12/bert_config.json'\n",
    "checkpoint_path = './bert_weight_file/uncased_L-4_H-768_A-12/bert_model.ckpt'\n",
    "\n",
    "# 模型保存路径\n",
    "model_save_path = './save_model/bert_bilstm_crf.weight'\n",
    "CRF_save_path = './save_model/CRF.npy'\n",
    "\n",
    "class NamedEntityRecognizer(ViterbiDecoder):\n",
    "    \"\"\"命名实体识别器\n",
    "    \"\"\"\n",
    "    def recognize(self, text):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        while len(tokens) > max_len:\n",
    "            tokens.pop(-2)\n",
    "        mapping = tokenizer.rematch(text, tokens)\n",
    "        token_ids = tokenizer.tokens_to_ids(tokens)\n",
    "        segment_ids = [0] * len(token_ids)\n",
    "        token_ids, segment_ids = to_array([token_ids], [segment_ids]) # ndarray\n",
    "        nodes = model.predict([token_ids, segment_ids])[0] # [sqe_len,23]\n",
    "        labels = self.decode(nodes) # id [sqe_len,], [0 0 0 0 0 7 8 8 0 0 0 0 0 0 0]\n",
    "        entities, starting = [], False\n",
    "        for i, label in enumerate(labels):\n",
    "            if label > 0:\n",
    "                if label % 2 == 1:\n",
    "                    starting = True\n",
    "                    entities.append([[i], id2label[(label - 1) // 2]])\n",
    "                elif starting:\n",
    "                    entities[-1][0].append(i)\n",
    "                else:\n",
    "                    starting = False\n",
    "            else:\n",
    "                starting = False\n",
    "        return [(text[mapping[w[0]][0]:mapping[w[-1]][-1] + 1], l) for w, l in entities]\n",
    "    \n",
    "#相等应加set（）中源文本的数量    \n",
    "def ner_metrics(data,fine_train_list):\n",
    "    X,Y,Z = 1e-6,1e-6,1e-6\n",
    "    count = 0\n",
    "    for d in tqdm(data):\n",
    "        text = ''.join([i[0] for i in d])\n",
    "        pred= NER.recognize(text)\n",
    "        R = set(pred)\n",
    "        T = set([tuple(i) for i in d if i[1] != 'O'])\n",
    "        \n",
    "        # 便于T和R做交集\n",
    "        m = []\n",
    "        for i in T:\n",
    "            for j in i[0]:\n",
    "                m.append((j,i[1]))\n",
    "        T = set(m)\n",
    "        \n",
    "        # 填充train_predict_list,更新fine_train_list\n",
    "        if len(T) > 0 :  \n",
    "            if len(train_predict_list) < 8275:\n",
    "                train_predict_list.append(R&T)\n",
    "            else:\n",
    "                if len(R&T) > fine_train_list[count]:\n",
    "#                     print('text: ',text)\n",
    "#                     print('T: ',T)\n",
    "#                     print('R&T: ',R&T)\n",
    "                    train_predict_list[count] = R&T\n",
    "            if len(R&T) > fine_train_list[count]:\n",
    "                fine_train_list[count] = len(R&T)\n",
    "            \n",
    "            \n",
    "#         if len(T) < fine_train_list[count]:\n",
    "#             print(False)\n",
    "#             print('text: ',text)\n",
    "#             print('T: ',T)\n",
    "#             print('R&T: ',R&T)\n",
    "#             print('fine_train_list[count]: ',fine_train_list[count])\n",
    "#             print()\n",
    "\n",
    "        X += fine_train_list[count]\n",
    "        if len(R) < fine_train_list[count]:\n",
    "            Y += fine_train_list[count]\n",
    "        else:\n",
    "            Y += len(R)\n",
    "            \n",
    "        Z += len(T)\n",
    "        count += 1\n",
    "\n",
    "    f1,precision,recall = 2 * X / (Y + Z),X / Y,X / Z\n",
    "    return f1,precision,recall\n",
    "\n",
    "class Evaluator(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(Evaluator, self).__init__()\n",
    "        self.best_val_f1 = 0\n",
    "    def on_epoch_end(self, epoch,logs=None):\n",
    "        NER.trans = K.eval(CRF.trans) # 可能有错\n",
    "        f1, precision, recall = ner_metrics(valid_data,fine_train_list)\n",
    "        if f1 > self.best_val_f1:\n",
    "            model.save_weights(model_save_path)\n",
    "            self.best_val_f1 = f1\n",
    "            print('save model to {}'.format(checkpoint_path))\n",
    "        else:\n",
    "            global learning_rate\n",
    "            learning_rate = learning_rate / 5\n",
    "        print(\n",
    "              'valid: f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f\\n' %\n",
    "              (f1,precision,recall,self.best_val_f1)\n",
    "        )\n",
    "        \n",
    "# def adversarial_training(model, embedding_name, epsilon=1):\n",
    "#     \"\"\"\n",
    "#     给模型添加对抗训练\n",
    "#     其中model是需要添加对抗训练的keras模型\n",
    "#     \"\"\"\n",
    "#     if model.train_function is None:  # 如果还没有训练函数\n",
    "#         model._make_train_function()  # 手动make\n",
    "#     old_train_function = model.train_function  # 备份旧的训练函数\n",
    "\n",
    "#     # 查找Embedding层\n",
    "#     for output in model.outputs:\n",
    "#         embedding_layer = search_layer(output, embedding_name)\n",
    "#         if embedding_layer is not None:\n",
    "#             break\n",
    "#     if embedding_layer is None:\n",
    "#         raise Exception('Embedding layer not found')\n",
    "\n",
    "#     # 求Embedding梯度\n",
    "#     embeddings = embedding_layer.embeddings  # Embedding矩阵\n",
    "#     gradients = K.gradients(model.total_loss, [embeddings])  # Embedding梯度\n",
    "#     gradients = K.zeros_like(embeddings) + gradients[0]  # 转为dense tensor\n",
    "\n",
    "#     # 封装为函数\n",
    "#     inputs = (\n",
    "#         model._feed_inputs + model._feed_targets + model._feed_sample_weights\n",
    "#     )  # 所有输入层\n",
    "#     embedding_gradients = K.function(\n",
    "#         inputs=inputs,\n",
    "#         outputs=[gradients],\n",
    "#         name='embedding_gradients',\n",
    "#     )  # 封装为函数\n",
    "\n",
    "#     def train_function(inputs):\n",
    "#         # 重新定义训练函数\n",
    "#         grads = embedding_gradients(inputs)[0]  # Embedding梯度\n",
    "#         delta = epsilon * grads / (np.sqrt((grads**2).sum()) + 1e-8)  # 计算扰动\n",
    "#         K.set_value(embeddings, K.eval(embeddings) + delta)  # 注入扰动\n",
    "#         outputs = old_train_function(inputs)  # 梯度下降\n",
    "#         K.set_value(embeddings, K.eval(embeddings) - delta)  # 删除扰动\n",
    "#         return outputs\n",
    "#     model.train_function = train_function  # 覆盖原训练函数        \n",
    "\n",
    "\n",
    "\n",
    "model,CRF = bert_bilstm_crf(config_path,checkpoint_path,num_labels,lstm_units,drop_rate,learning_rate)\n",
    "# adversarial_training(model,'Embedding-Token',0.5)\n",
    "NER = NamedEntityRecognizer(trans=K.eval(CRF.trans), starts=[0], ends=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['医生：你好我是您的接诊医生', 'O']], [['医生：宝贝最近吃奶量可以吗？下降了吗', 'O']], [['患者：没有，也没怎么', 'O'], ['哭闹', 'Symptom']], [['医生：宝妈有没有吃生冷辛辣刺激食物油腻食物来吗？', 'O']], [['医生：宝贝奶粉的话最近换过牌子吗？', 'O']], [['医生：宝贝肚子着凉来吗？', 'O']], [['患者：喝茶油腻也少，菜吃很多', 'O']], [['医生：嗯嗯，宝妈饮食一定注意，生冷辛辣刺激食物不能吃油腻食物不能吃，特别油腻食物的奥，清淡饮食为主，这个时候宝贝胃肠功能可能会有影响，能吃多少吃多少别强喂的奥！', 'O']], [['医生：宝贝最近有没有', 'O'], ['呕吐', 'Symptom'], ['症状呢？', 'O']], [['患者：', 'O'], ['呕吐', 'Symptom'], ['，有时会', 'O'], ['吐', 'Symptom'], ['，不多', 'O']]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_data,_ = load_data('./ner_data/train/train.txt',128)\n",
    "    valid_data,_ = load_data('./ner_data/dev/test.txt',128)\n",
    "    \n",
    "\n",
    "    print(train_data[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/4\n",
      "3718/3718 [==============================] - 8409s 2s/step - loss: 0.1477 - sparse_accuracy: 0.9973 - val_loss: 1.5178 - val_sparse_accuracy: 0.9775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [03:31<00:00, 39.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model to ./bert_weight_file/uncased_L-4_H-768_A-12/bert_model.ckpt\n",
      "valid: f1: 0.91758, precision: 0.86984, recall: 0.97086, best f1: 0.91758\n",
      "\n",
      "Epoch 2/4\n",
      "3718/3718 [==============================] - 8430s 2s/step - loss: 0.1211 - sparse_accuracy: 0.9978 - val_loss: 0.8396 - val_sparse_accuracy: 0.9630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [03:30<00:00, 39.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: f1: 0.86092, precision: 0.77163, recall: 0.97357, best f1: 0.91758\n",
      "\n",
      "Epoch 3/4\n",
      "3718/3718 [==============================] - 8450s 2s/step - loss: 0.0535 - sparse_accuracy: 0.9989 - val_loss: 0.8199 - val_sparse_accuracy: 0.9744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [03:26<00:00, 40.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: f1: 0.89976, precision: 0.83598, recall: 0.97408, best f1: 0.91758\n",
      "\n",
      "Epoch 4/4\n",
      "3718/3718 [==============================] - 8396s 2s/step - loss: 0.0248 - sparse_accuracy: 0.9994 - val_loss: 2.3253 - val_sparse_accuracy: 0.9774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [03:27<00:00, 39.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: f1: 0.90911, precision: 0.85226, recall: 0.97408, best f1: 0.91758\n",
      "\n",
      "[[ 0.57122225 -0.6133416  -0.0752689   0.11904959 -0.41914526 -0.5894722\n",
      "  -0.48531607 -0.34697372 -0.05795071 -0.6759177  -0.3357175 ]\n",
      " [-0.3182494  -0.2311095  -0.37499154  0.2069319   0.19512522 -0.5219225\n",
      "  -0.3076908  -0.22989158  0.2468979  -0.01747333 -0.08698532]\n",
      " [-0.5956915  -0.24547535 -0.18386608 -0.3560527  -0.36180395 -0.33080438\n",
      "   0.45021257 -0.05060589 -0.49123704 -0.14037913 -0.17109317]\n",
      " [-0.61277467  0.04253093  0.21743886 -0.00321927 -0.0349032   0.00765893\n",
      "   0.18040866  0.00577142 -0.2869069  -0.64182186 -0.1522031 ]\n",
      " [-0.65195215 -0.21891981  0.37502876 -0.19458427  0.07435843 -0.49571252\n",
      "  -0.51378894  0.12189484  0.4857217  -0.39254132  0.51117957]\n",
      " [ 0.30123577  0.3590206   0.11038376  0.14211111 -0.14814666 -0.2882339\n",
      "  -0.33709186 -0.50399256 -0.60252005 -0.01256281 -0.3170192 ]\n",
      " [-0.7756505   0.08338065  0.26491472 -0.35381716  0.34544346 -0.21493115\n",
      "  -0.02121825 -0.06578448  0.22987625 -0.7425756  -0.10307115]\n",
      " [-0.25087836 -0.08023655  0.38477674  0.21150447 -0.5123843   0.2451335\n",
      "  -0.09253601  0.36848894 -0.31838688  0.04028199 -0.25240946]\n",
      " [ 0.13126197 -0.33974332 -0.29849872 -0.40091607 -0.50157577 -0.5475593\n",
      "  -0.17936447 -0.3844171   0.32226577  0.04620696  0.01705058]\n",
      " [-0.57910544  0.22158563 -0.60851806 -0.41100955 -0.50260204 -0.08798529\n",
      "  -0.21488962 -0.22905502  0.00981362  0.55043006 -0.7026842 ]\n",
      " [-0.15983176  0.1916705   0.29045323  0.32823423 -0.48059052 -0.6115002\n",
      "   0.2931604   0.31834608 -0.1625439   0.05907715  0.16588509]]\n",
      "(11, 11)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_data,_ = load_data('./ner_data/train/train.txt',128)\n",
    "    valid_data,_ = load_data('./ner_data/dev/test.txt',128)\n",
    "    \n",
    "    flag = False\n",
    "    count = 0\n",
    "    i = 0\n",
    "    while(i<len(train_data)):\n",
    "        if flag==True:\n",
    "            i = i-1\n",
    "        if train_data[i][0][1] == 'O'and len(train_data[i])==1:\n",
    "            del train_data[i]\n",
    "            flag = True\n",
    "            count+=1\n",
    "        else:\n",
    "            for j in range(count):\n",
    "                train_data.append(train_data[i])\n",
    "            flag = False\n",
    "            count = 0\n",
    "        i += 1\n",
    "    \n",
    "    train_generator = data_generator(train_data, batch_size)\n",
    "    valid_generator = data_generator(valid_data, batch_size*5)\n",
    "    \n",
    "    evaluator = Evaluator()\n",
    "    \n",
    "    def scheduler(epoch):\n",
    "        return learning_rate/(max(2*(epoch-1),1))\n",
    "\n",
    "    lr_scheduler = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    \n",
    "    model.fit(\n",
    "        train_generator.forfit(),\n",
    "        steps_per_epoch = len(train_generator),\n",
    "        validation_data = valid_generator.forfit(),\n",
    "        validation_steps = len(valid_generator),\n",
    "        epochs = epochs,\n",
    "        callbacks = [evaluator,lr_scheduler]\n",
    "    )\n",
    "    \n",
    "    print(K.eval(CRF.trans))\n",
    "    print(K.eval(CRF.trans).shape)\n",
    "    model.save_weights(model_save_path)\n",
    "    np.save(CRF_save_path, K.eval(CRF.trans))\n",
    "\n",
    "    # torch.save(model, model_save_path)\n",
    "    # pickle.dump(K.eval(CRF.trans),open('./save_model/crf_trans.pkl','rb'))\n",
    "    \n",
    "else:\n",
    "    # model = torch.load(model_save_path)\n",
    "    model.load_weights(model_save_path)\n",
    "    # NER.trans = pickle.load(open('./save_model/crf_trans.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 8, 3, 0, 0, 0, 0, 2, 0, 0, 4, 0, 1, 1, 0, 4, 2, 0, 0, 0, 0, 4, 0, 0, 0, 0, 2, 2]\n",
      "[set(), {('肚', 'Symptom'), ('拉', 'Symptom'), ('子', 'Symptom')}, set(), set(), set(), set(), set(), set(), set(), {('肚', 'Symptom'), ('拉', 'Symptom'), ('子', 'Symptom')}]\n"
     ]
    }
   ],
   "source": [
    "print(fine_train_list[0:100])\n",
    "print(train_predict_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 保存矩阵\n",
    "fine=np.array(fine_train_list)\n",
    "tpl = np.array(train_predict_list)\n",
    "np.save('./fine_train_list.npy',fine)\n",
    "np.save('./train_predict_list.npy',tpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载矩阵\n",
    "fine = np.load('./fine_train_list.npy')\n",
    "fine_train_list = fine.tolist()\n",
    "tpl = np.load('./train_predict_list.npy',allow_pickle=True)\n",
    "train_predict_list = tpl.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/2\n",
      "3718/3718 [==============================] - 7338s 2s/step - loss: 0.1314 - sparse_accuracy: 0.9961 - val_loss: 0.5614 - val_sparse_accuracy: 0.9870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [03:31<00:00, 39.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model to ./bert_weight_file/uncased_L-4_H-768_A-12/bert_model.ckpt\n",
      "valid: f1: 0.95462, precision: 0.93437, recall: 0.97576, best f1: 0.95462\n",
      "\n",
      "Epoch 2/2\n",
      "3718/3718 [==============================] - 7301s 2s/step - loss: 0.1562 - sparse_accuracy: 0.9951 - val_loss: 0.7796 - val_sparse_accuracy: 0.9860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8275/8275 [03:27<00:00, 39.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: f1: 0.94940, precision: 0.92415, recall: 0.97606, best f1: 0.95462\n",
      "\n",
      "[[ 0.645299   -0.7370785  -0.56542546  0.00503931 -0.90727884 -0.68640953\n",
      "  -0.9690469  -0.39257857 -0.5478962  -0.8409375  -0.8255535 ]\n",
      " [-0.4483786  -0.15622358 -0.6728855   0.10875657 -0.1081326  -0.6022599\n",
      "  -0.60791546 -0.27157855 -0.05456834 -0.07525794 -0.39097717]\n",
      " [-1.0685419  -0.55251616 -0.18916054 -0.59706146 -0.36600417 -0.6172872\n",
      "   0.4415291  -0.17096259 -0.4977     -0.5430737  -0.17884265]\n",
      " [-0.7189193  -0.073939   -0.01607681  0.06212772 -0.26827216 -0.0518255\n",
      "  -0.05279429 -0.05428631 -0.51619667 -0.7174855  -0.38692203]\n",
      " [-1.1253781  -0.5278463   0.36783957 -0.4331767   0.06927758 -0.7758725\n",
      "  -0.51649743  0.00273154  0.47274798 -0.78524625  0.49940804]\n",
      " [ 0.1995825   0.28520766 -0.13622653  0.07774224 -0.39409238 -0.20836465\n",
      "  -0.5750944  -0.6027726  -0.8602776  -0.08078734 -0.5711158 ]\n",
      " [-1.2439601  -0.22419953  0.25872597 -0.5886075   0.33913046 -0.49701262\n",
      "  -0.02538386 -0.18118031  0.22006576 -1.1278424  -0.10936534]\n",
      " [-0.28936616 -0.1317855   0.2752406   0.1786026  -0.6100805   0.16847913\n",
      "  -0.19514954  0.39591062 -0.4253762  -0.01011426 -0.35858622]\n",
      " [-0.35243505 -0.6497472  -0.3033082  -0.6431068  -0.5052889  -0.8354262\n",
      "  -0.1841727  -0.5022886   0.3081041  -0.3569765   0.00763498]\n",
      " [-0.74334997  0.13695937 -0.9938951  -0.49730003 -0.88967884 -0.16784437\n",
      "  -0.6025193  -0.27090737 -0.39663237  0.6345032  -1.0946826 ]\n",
      " [-0.63693625 -0.12023345  0.28381371  0.08288404 -0.48354632 -0.8942681\n",
      "   0.2872081   0.19526634 -0.16957603 -0.34282875  0.15737437]]\n",
      "(11, 11)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epochs = 2\n",
    "    \n",
    "    train_data,_ = load_data('./ner_data/train/train.txt',128)\n",
    "    valid_data,_ = load_data('./ner_data/dev/test.txt',128)\n",
    "    \n",
    "\n",
    "    train_generator = data_generator(train_data, batch_size)\n",
    "    valid_generator = data_generator(valid_data, batch_size*5)\n",
    "    \n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        model_save_path,\n",
    "        monitor = 'val_sparse_accuracy',\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        mode = 'max'\n",
    "    )\n",
    "    evaluator = Evaluator()\n",
    "    \n",
    "#     def scheduler(epoch):\n",
    "#         return learning_rate/(max(2*(epoch-1),1))\n",
    "\n",
    "#     lr_scheduler = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    \n",
    "    model.fit(\n",
    "        train_generator.forfit(),\n",
    "        steps_per_epoch = len(train_generator),\n",
    "        validation_data = valid_generator.forfit(),\n",
    "        validation_steps = len(valid_generator),\n",
    "        epochs = epochs,\n",
    "        callbacks = [evaluator]\n",
    "    )\n",
    "    \n",
    "    print(K.eval(CRF.trans))\n",
    "    print(K.eval(CRF.trans).shape)\n",
    "    model.save_weights(model_save_path)\n",
    "    np.save(CRF_save_path, K.eval(CRF.trans))\n",
    "\n",
    "    # torch.save(model, model_save_path)\n",
    "    # pickle.dump(K.eval(CRF.trans),open('./save_model/crf_trans.pkl','rb'))\n",
    "    \n",
    "else:\n",
    "    # model = torch.load(model_save_path)\n",
    "    model.load_weights(model_save_path)\n",
    "    # NER.trans = pickle.load(open('./save_model/crf_trans.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('么', 'Medical_Examination'),\n",
       " ('可', 'Medical_Examination'),\n",
       " ('题', 'Medical_Examination')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {('者', 'Drug'), ('六', 'Medical_Examination'), ('今', 'Medical_Examination'), ('医院，', 'Medical_Examination'), ('药', 'Medical_Examination'), ('服医院的', 'Medical_Examination'), ('口', 'Medical_Examination'), ('：', 'Medical_Examination'), ('天', 'Medical_Examination'), ('去的', 'Medical_Examination'), ('次', 'Medical_Examination'), ('是', 'Medical_Examination'), ('中', 'Medical_Examination'), ('明', 'Medical_Examination'), ('周', 'Medical_Examination'), ('午', 'Medical_Examination'), ('上', 'Medical_Examination')}\n",
    "v = {('题', 'Medical_Examination'), ('么', 'Medical_Examination'), ('可', 'Medical_Examination'), ('留', 'Medical_Examination'), ('以给我', 'Medical_Examination'), ('什', 'Medical_Examination'), ('问', 'Medical_Examination')}\n",
    "c = []\n",
    "m = {('题', 'Medical_Examination'), ('么', 'Medical_Examination'), ('可', 'Medical_Examination')}\n",
    "h = set()\n",
    "c.append(a)\n",
    "c.append(m)\n",
    "c.append(v)\n",
    "c.append(h)\n",
    "c[1] & c[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{('么', 'Medical_Examination'),\n",
       "  ('可', 'Medical_Examination'),\n",
       "  ('题', 'Medical_Examination')},\n",
       " {('么', 'Medical_Examination'),\n",
       "  ('可', 'Medical_Examination'),\n",
       "  ('题', 'Medical_Examination')},\n",
       " {('么', 'Medical_Examination'),\n",
       "  ('什', 'Medical_Examination'),\n",
       "  ('以给我', 'Medical_Examination'),\n",
       "  ('可', 'Medical_Examination'),\n",
       "  ('留', 'Medical_Examination'),\n",
       "  ('问', 'Medical_Examination'),\n",
       "  ('题', 'Medical_Examination')},\n",
       " set()]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0] = {('题', 'Medical_Examination'), ('么', 'Medical_Examination'), ('可', 'Medical_Examination')}\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习率分层：f1：0.40678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.0076, precision_score: 0.0065, recall_score: 0.0089, accuracy_score: 0.8839\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "            Symptom     0.0151    0.0133    0.0142      3007\n",
      "      Drug_Category     0.0000    0.0000    0.0000       552\n",
      "               Drug     0.0000    0.0000    0.0000       658\n",
      "Medical_Examination     0.0063    0.0083    0.0072       846\n",
      "          Operation     0.0000    0.0000    0.0000       189\n",
      "\n",
      "          micro avg     0.0065    0.0089    0.0076      5252\n",
      "          macro avg     0.0097    0.0089    0.0093      5252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_entities(seq, suffix=False):\n",
    "    \"\"\"Gets entities from sequence.\n",
    "\n",
    "    Args:\n",
    "        seq (list): sequence of labels.\n",
    "\n",
    "    Returns:\n",
    "        list: list of (chunk_type, chunk_start, chunk_end).\n",
    "\n",
    "    Example:\n",
    "        # >>> from seqeval.metrics.sequence_labeling import get_entities\n",
    "        # >>> seq = ['B-PER', 'I-PER', 'O', 'B-LOC']\n",
    "        # >>> get_entities(seq)\n",
    "        [('PER', 0, 1), ('LOC', 3, 3)]\n",
    "    \"\"\"\n",
    "    # for nested list\n",
    "    if any(isinstance(s, list) for s in seq):\n",
    "        seq = [item for sublist in seq for item in sublist + ['O']]\n",
    "\n",
    "    prev_tag = 'O'\n",
    "    prev_type = ''\n",
    "    begin_offset = 0\n",
    "    chunks = []\n",
    "    # print(seq)\n",
    "    for i, chunk in enumerate(seq + ['O']):\n",
    "        # print(i,chunk)\n",
    "        if suffix:\n",
    "            tag = chunk[-1]\n",
    "            type_ = chunk.split('-')[0]\n",
    "        else:\n",
    "            try:\n",
    "                tag = chunk[0]\n",
    "                type_ = chunk.split('-')[-1]\n",
    "            except IndexError:\n",
    "                tag = 'O'\n",
    "                type_ = 'O'\n",
    "        if end_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "            chunks.append((prev_type, begin_offset, i-1))\n",
    "        if start_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "            begin_offset = i\n",
    "        prev_tag = tag\n",
    "        prev_type = type_\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def end_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    \"\"\"Checks if a chunk ended between the previous and current word.\n",
    "\n",
    "    Args:\n",
    "        prev_tag: previous chunk tag.\n",
    "        tag: current chunk tag.\n",
    "        prev_type: previous type.\n",
    "        type_: current type.\n",
    "\n",
    "    Returns:\n",
    "        chunk_end: boolean.\n",
    "    \"\"\"\n",
    "    chunk_end = False\n",
    "\n",
    "    if prev_tag == 'E': chunk_end = True\n",
    "    if prev_tag == 'S': chunk_end = True\n",
    "\n",
    "    if prev_tag == 'B' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'S': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'O': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'S': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'O': chunk_end = True\n",
    "\n",
    "    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n",
    "        chunk_end = True\n",
    "\n",
    "    return chunk_end\n",
    "\n",
    "\n",
    "def start_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    \"\"\"Checks if a chunk started between the previous and current word.\n",
    "\n",
    "    Args:\n",
    "        prev_tag: previous chunk tag.\n",
    "        tag: current chunk tag.\n",
    "        prev_type: previous type.\n",
    "        type_: current type.\n",
    "\n",
    "    Returns:\n",
    "        chunk_start: boolean.\n",
    "    \"\"\"\n",
    "    chunk_start = False\n",
    "\n",
    "    if tag == 'B': chunk_start = True\n",
    "    if tag == 'S': chunk_start = True\n",
    "\n",
    "    if prev_tag == 'E' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'E' and tag == 'I': chunk_start = True\n",
    "    if prev_tag == 'S' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'S' and tag == 'I': chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'I': chunk_start = True\n",
    "\n",
    "    if tag != 'O' and tag != '.' and prev_type != type_:\n",
    "        chunk_start = True\n",
    "\n",
    "    return chunk_start\n",
    "\n",
    "\n",
    "def f1_score(y_true: object, y_pred: object, average: object = 'micro', suffix: object = False) -> object:\n",
    "    \"\"\"Compute the F1 score.\n",
    "\n",
    "    The F1 score can be interpreted as a weighted average of the precision and\n",
    "    recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "    The relative contribution of precision and recall to the F1 score are\n",
    "    equal. The formula for the F1 score is::\n",
    "\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    Args:\n",
    "        y_true : 2d array. Ground truth (correct) target values.\n",
    "        y_pred : 2d array. Estimated targets as returned by a tagger.\n",
    "\n",
    "    Returns:\n",
    "        score : float.\n",
    "\n",
    "    Example:\n",
    "        # >>> from seqeval.metrics import f1_score\n",
    "        # >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "        # >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "        # >>> f1_score(y_true, y_pred)\n",
    "        0.50\n",
    "    \"\"\"\n",
    "    true_entities = set(get_entities(y_true, suffix))\n",
    "    pred_entities = set(get_entities(y_pred, suffix))\n",
    "\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_pred = len(pred_entities)\n",
    "    nb_true = len(true_entities)\n",
    "\n",
    "    p = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "    r = nb_correct / nb_true if nb_true > 0 else 0\n",
    "    score = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"Accuracy classification score.\n",
    "\n",
    "    In multilabel classification, this function computes subset accuracy:\n",
    "    the set of labels predicted for a sample must *exactly* match the\n",
    "    corresponding set of labels in y_true.\n",
    "\n",
    "    Args:\n",
    "        y_true : 2d array. Ground truth (correct) target values.\n",
    "        y_pred : 2d array. Estimated targets as returned by a tagger.\n",
    "\n",
    "    Returns:\n",
    "        score : float.\n",
    "\n",
    "    Example:\n",
    "        # >>> from seqeval.metrics import accuracy_score\n",
    "        # >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "        # >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "        # >>> accuracy_score(y_true, y_pred)\n",
    "        0.80\n",
    "    \"\"\"\n",
    "    if any(isinstance(s, list) for s in y_true):\n",
    "        y_true = [item for sublist in y_true for item in sublist]\n",
    "        y_pred = [item for sublist in y_pred for item in sublist]\n",
    "\n",
    "    nb_correct = sum(y_t==y_p for y_t, y_p in zip(y_true, y_pred))\n",
    "    nb_true = len(y_true)\n",
    "\n",
    "    score = nb_correct / nb_true\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def precision_score(y_true, y_pred, average='micro', suffix=False):\n",
    "    \"\"\"Compute the precision.\n",
    "\n",
    "    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
    "    true positives and ``fp`` the number of false positives. The precision is\n",
    "    intuitively the ability of the classifier not to label as positive a sample.\n",
    "\n",
    "    The best value is 1 and the worst value is 0.\n",
    "\n",
    "    Args:\n",
    "        y_true : 2d array. Ground truth (correct) target values.\n",
    "        y_pred : 2d array. Estimated targets as returned by a tagger.\n",
    "\n",
    "    Returns:\n",
    "        score : float.\n",
    "\n",
    "    Example:\n",
    "        # >>> from seqeval.metrics import precision_score\n",
    "        # >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "        # >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "        # >>> precision_score(y_true, y_pred)\n",
    "        0.50\n",
    "    \"\"\"\n",
    "    true_entities = set(get_entities(y_true, suffix))\n",
    "    pred_entities = set(get_entities(y_pred, suffix))\n",
    "\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_pred = len(pred_entities)\n",
    "\n",
    "    score = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def recall_score(y_true, y_pred, average='micro', suffix=False):\n",
    "    \"\"\"Compute the recall.\n",
    "\n",
    "    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
    "    true positives and ``fn`` the number of false negatives. The recall is\n",
    "    intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "    The best value is 1 and the worst value is 0.\n",
    "\n",
    "    Args:\n",
    "        y_true : 2d array. Ground truth (correct) target values.\n",
    "        y_pred : 2d array. Estimated targets as returned by a tagger.\n",
    "\n",
    "    Returns:\n",
    "        score : float.\n",
    "\n",
    "    Example:\n",
    "        # >>> from seqeval.metrics import recall_score\n",
    "        # >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "        # >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "        # >>> recall_score(y_true, y_pred)\n",
    "        0.50\n",
    "    \"\"\"\n",
    "    true_entities = set(get_entities(y_true, suffix))\n",
    "    pred_entities = set(get_entities(y_pred, suffix))\n",
    "\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_true = len(true_entities)\n",
    "\n",
    "    score = nb_correct / nb_true if nb_true > 0 else 0\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def performance_measure(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the performance metrics: TP, FP, FN, TN\n",
    "\n",
    "    Args:\n",
    "        y_true : 2d array. Ground truth (correct) target values.\n",
    "        y_pred : 2d array. Estimated targets as returned by a tagger.\n",
    "\n",
    "    Returns:\n",
    "        performance_dict : dict\n",
    "\n",
    "    Example:\n",
    "        # >>> from seqeval.metrics import performance_measure\n",
    "        # >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'B-ORG'], ['B-PER', 'I-PER', 'O']]\n",
    "        # >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "        # >>> performance_measure(y_true, y_pred)\n",
    "        (3, 3, 1, 4)\n",
    "    \"\"\"\n",
    "    performace_dict = dict()\n",
    "    if any(isinstance(s, list) for s in y_true):\n",
    "        y_true = [item for sublist in y_true for item in sublist]\n",
    "        y_pred = [item for sublist in y_pred for item in sublist]\n",
    "    performace_dict['TP'] = sum(y_t == y_p for y_t, y_p in zip(y_true, y_pred)\n",
    "                                if ((y_t != 'O') or (y_p != 'O')))\n",
    "    performace_dict['FP'] = sum(y_t != y_p for y_t, y_p in zip(y_true, y_pred))\n",
    "    performace_dict['FN'] = sum(((y_t != 'O') and (y_p == 'O'))\n",
    "                                for y_t, y_p in zip(y_true, y_pred))\n",
    "    performace_dict['TN'] = sum((y_t == y_p == 'O')\n",
    "                                for y_t, y_p in zip(y_true, y_pred))\n",
    "\n",
    "    return performace_dict\n",
    "\n",
    "\n",
    "def classification_report(y_true, y_pred, digits=2, suffix=False):\n",
    "    \"\"\"Build a text report showing the main classification metrics.\n",
    "\n",
    "    Args:\n",
    "        y_true : 2d array. Ground truth (correct) target values.\n",
    "        y_pred : 2d array. Estimated targets as returned by a classifier.\n",
    "        digits : int. Number of digits for formatting output floating point values.\n",
    "\n",
    "    Returns:\n",
    "        report : string. Text summary of the precision, recall, F1 score for each class.\n",
    "\n",
    "    Examples:\n",
    "        # >>> from seqeval.metrics import classification_report\n",
    "        # >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "        # >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "        # >>> print(classification_report(y_true, y_pred))\n",
    "                     precision    recall  f1-score   support\n",
    "        <BLANKLINE>\n",
    "               MISC       0.00      0.00      0.00         1\n",
    "                PER       1.00      1.00      1.00         1\n",
    "        <BLANKLINE>\n",
    "          micro avg       0.50      0.50      0.50         2\n",
    "          macro avg       0.50      0.50      0.50         2\n",
    "        <BLANKLINE>\n",
    "    \"\"\"\n",
    "    true_entities = set(get_entities(y_true, suffix))\n",
    "    pred_entities = set(get_entities(y_pred, suffix))\n",
    "\n",
    "    name_width = 0\n",
    "    d1 = defaultdict(set)\n",
    "    d2 = defaultdict(set)\n",
    "    for e in true_entities:\n",
    "        d1[e[0]].add((e[1], e[2]))\n",
    "        name_width = max(name_width, len(e[0]))\n",
    "    for e in pred_entities:\n",
    "        d2[e[0]].add((e[1], e[2]))\n",
    "\n",
    "    last_line_heading = 'macro avg'\n",
    "    width = max(name_width, len(last_line_heading), digits)\n",
    "\n",
    "    headers = [\"precision\", \"recall\", \"f1-score\", \"support\"]\n",
    "    head_fmt = u'{:>{width}s} ' + u' {:>9}' * len(headers)\n",
    "    report = head_fmt.format(u'', *headers, width=width)\n",
    "    report += u'\\n\\n'\n",
    "\n",
    "    row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' * 3 + u' {:>9}\\n'\n",
    "\n",
    "    ps, rs, f1s, s = [], [], [], []\n",
    "    for type_name, true_entities in d1.items():\n",
    "        pred_entities = d2[type_name]\n",
    "        nb_correct = len(true_entities & pred_entities)\n",
    "        nb_pred = len(pred_entities)\n",
    "        nb_true = len(true_entities)\n",
    "\n",
    "        p = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "        r = nb_correct / nb_true if nb_true > 0 else 0\n",
    "        f1 = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "        report += row_fmt.format(*[type_name, p, r, f1, nb_true], width=width, digits=digits)\n",
    "\n",
    "        ps.append(p)\n",
    "        rs.append(r)\n",
    "        f1s.append(f1)\n",
    "        s.append(nb_true)\n",
    "\n",
    "    report += u'\\n'\n",
    "\n",
    "    # compute averages\n",
    "    report += row_fmt.format('micro avg',\n",
    "                             precision_score(y_true, y_pred, suffix=suffix),\n",
    "                             recall_score(y_true, y_pred, suffix=suffix),\n",
    "                             f1_score(y_true, y_pred, suffix=suffix),\n",
    "                             np.sum(s),\n",
    "                             width=width, digits=digits)\n",
    "    report += row_fmt.format(last_line_heading,\n",
    "                             np.average(ps, weights=s),\n",
    "                             np.average(rs, weights=s),\n",
    "                             np.average(f1s, weights=s),\n",
    "                             np.sum(s),\n",
    "                             width=width, digits=digits)\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "def report_span_accuracy(_true, _pred):\n",
    "    \"\"\"\n",
    "    calculate span accuracy, namely ignore class label. Just check whether the predicted span is right.\n",
    "    :param _true:\n",
    "    :param _pred:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    y_true, y_pred = [], []\n",
    "    for _t in _true:\n",
    "        y_true.append([tag if tag == 'O' else '%s-d' % tag[0] for tag in _t])  # add dummy class-label: B-a --> B-d\n",
    "    for _p in _pred:\n",
    "        y_pred.append([tag if tag == 'O' else '%s-d' % tag[0] for tag in _p])  # add dummy class-label: B-a --> B-d\n",
    "    print('Span accuracy:')\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    " \n",
    "    \n",
    "def load_eval_data(data_path,max_len):\n",
    "    X = []\n",
    "    y = []\n",
    "    sentence = []\n",
    "    labels = []\n",
    "    split_pattern = re.compile(r'[；;。，、？！\\.\\?,! ]')\n",
    "    with open(data_path,'r',encoding = 'utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            #每行为一个字符和其tag，中间用tab或者空格隔开\n",
    "            # sentence = [w1,w2,w3,...,wn], labels=[B-xx,I-xxx,,,...,O]\n",
    "            line = line.strip().split()\n",
    "            if(not line or len(line) < 2): \n",
    "                X.append(sentence)\n",
    "                y.append(labels)\n",
    "                sentence = []\n",
    "                labels = []\n",
    "                continue\n",
    "            #word, tag = line[0], line[1].replace('_','-').replace('M','I').replace('E','I').replace('S','B') # BMES -> BIO\n",
    "            word, tag = line[0], line[1]\n",
    "            if split_pattern.match(word) and len(sentence)+8 >= max_len:\n",
    "                sentence.append(word)\n",
    "                labels.append(tag)\n",
    "                X.append(sentence)\n",
    "                y.append(labels)\n",
    "                sentence = []\n",
    "                labels = []\n",
    "            else:\n",
    "                sentence.append(word)\n",
    "                labels.append(tag)\n",
    "    if len(sentence):\n",
    "        X.append(sentence)\n",
    "        sentence = []\n",
    "        y.append(labels)\n",
    "        labels = []\n",
    "    return X,y\n",
    "\n",
    "# def predict_label(data,y_true):\n",
    "#     y_pred = []\n",
    "#     for d in data:\n",
    "#         text = ''.join([i[0] for i in d])\n",
    "#         entity_mentions = NER.recognize(text)\n",
    "#         pred = ['O' for _ in range(len(text))]\n",
    "#         b = 0\n",
    "\n",
    "#         for i in range(1,len(entity_mentions[1])-1):\n",
    "#             item = entity_mentions[1][i]\n",
    "#             print(item)\n",
    "#             word,typ = item[0],item[1]\n",
    "#             print(word)\n",
    "#             start = text.where(word,b)\n",
    "#             end = start + len(word)\n",
    "#             pred[start] = 'B-' + typ\n",
    "#             for i in range(start + 1, end):\n",
    "#                 pred[i] = 'I-' + typ\n",
    "#             b += len(word)\n",
    "#         y_pred.append(pred)\n",
    "\n",
    "#     return y_pred\n",
    "def predict_label(data,y_true):\n",
    "    y_pred = []\n",
    "    for d in data:\n",
    "        text = ''.join([i[0] for i in d])\n",
    "        entity_mentions = NER.recognize(text)\n",
    "        # print(\"entity_mentions\",entity_mentions)\n",
    "        pred = ['O' for _ in range(len(text))]\n",
    "        b = 0\n",
    "        for item in entity_mentions:\n",
    "            word,typ = item[0],item[1]\n",
    "            start = text.find(word,b)\n",
    "            end = start + len(word)\n",
    "            pred[start] = 'B-' + typ\n",
    "            for i in range(start + 1, end):\n",
    "                pred[i] = 'I-' + typ\n",
    "            b += len(word)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def evaluate():\n",
    "    \n",
    "    eval_path = './ner_data/dev/test.txt'\n",
    "    test_data,y_true = load_eval_data(eval_path,max_len)\n",
    "    y_pred = predict_label(test_data,y_true)\n",
    "\n",
    "    f1 = f1_score(y_true,y_pred)\n",
    "    p = precision_score(y_true,y_pred)\n",
    "    r = recall_score(y_true,y_pred)\n",
    "\n",
    "    acc = accuracy_score(y_true,y_pred)\n",
    "\n",
    "    print(\"f1_score: {:.4f}, precision_score: {:.4f}, recall_score: {:.4f}, accuracy_score: {:.4f}\".format(f1,p,r,acc))\n",
    "    print(classification_report(y_true, y_pred, digits=4, suffix=False))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
